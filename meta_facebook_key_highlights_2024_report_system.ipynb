{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta (previously Facebook) Key Highlights 2024 Report System [❗️ Work In Progress]\n",
    "\n",
    "> Disclaimer 1 💡: This project is for educational, academic and research purposes. Please use this technology ethically, legally and responsibly.\n",
    "\n",
    "> Disclaimer 2 💡: This project is not officially affiliated with Meta or any of their partner organizations. It's entirely independent and created from ground-up by [AdiPat](https://www.github.com/AdiPat) at [The Hackers Playbook](https://www.thehackersplaybook.substack.com).\n",
    "\n",
    "Welcome! Before we begin, let's set some context. This will help you better understand the underlying motivation and intent of the project. Hopefully, it will motivate and inspire you to improve your programming skills and upgrade your abilities. I've designed this notebook to help you at every step, so if you ever get stuck, spend some time reading each line and folllowing each step exactly as prescribed and you will definitely find a solution. I can't accurately articulate how that works, but it works.\n",
    "\n",
    "## Personal Note\n",
    "\n",
    "- Firstly, this section is subjective. Feel free to ignore it. Whether you choose to believe or disbelieve the contents of this section, in either situation, the outcome of this project is independent of your most likely, well-intentioned judgement.\n",
    "\n",
    "- I've been following Mark Zuckerberg since I was in school, which is approximately since 2009/2010. I don't know the exact year but it was around this time when I learnt about him through the Internet.\n",
    "\n",
    "- We share several similarities in terms of personality, attitude towards life, and most importantly Programming. I don't know him personally so this is based on his public appearances and whatever I have heard or read about him online and from people. Assuming that all the information I gathered is true, then if the world were devoid of biological and cultural nuances, and we were all judged purely as Programmers, it would be fair to say that I'm like Mark Zuckerberg's younger \"programming brother\".\n",
    "\n",
    "- Most importantly, he was and is still a \"hacker\" at heart. If you don't believe me, observe every public announcement Meta makes.\n",
    "\n",
    "- I have never officially worked at Meta but I like to consider myself as an invisible and unofficial contributor (not employee) at Meta. I came up with this idea to motivate myself to keep up to the standards of the ever evolving tech world and to maintain a cultural identity of myself which closely resembles \"The Hacker Culture\".\n",
    "\n",
    "- There's a lot to add, but as time progresses, I'll continue talking about \"Mark Zuckerberg's influence on AdiPat's life\" in greater detail!\n",
    "\n",
    "If you ever have any questions, email us at `thehackersplaybook0@gmail.com`.\n",
    "\n",
    "## Naming Convention\n",
    "\n",
    "The file name and title includes Facebook for backward compatability with search engines and crawlers. Since this research project is Free & Open Source, it's important for the Hacker Culture that it reaches maximum number of people so that humanity can benefit from the efforts and energy directed into this endevour.\n",
    "\n",
    "## Goals & Objectives\n",
    "\n",
    "**Note:** Goals are broad, long-term aspirations that provide direction, while objectives are specific, measurable, and time-bound steps to achieve those goals.\n",
    "\n",
    "| Goals                                                                                                  | Objectives                                                                                                                                                    |\n",
    "| ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| To demonstrate the power of Llama 3.1 in AI-powered automated report generation for enterprises.       | To create a valuable content product for The Hackers Playbook which will help our students upskill.                                                           |\n",
    "| Open a discussion for official partnerships with Meta and their partner organizations.                 | Successfully utilize FireCrawl for information retrieval with measurable markers that can be used by FireCrawl (Mendable) to improve it's product experience. |\n",
    "| Contribute to Meta's Free & Open Source repository of software products. React inspires all of us.     | Contribute to the Python ecosystem. Create an executable experiment for Python Programmers to learn from.                                                     |\n",
    "| Extract key learnings from Meta's 2024 highlights to drive strategic insights at The Hackers Playbook. | Highlight AI-safety, AI-security, Ethical AI and Responsible AI practises in action.                                                                          |\n",
    "| Encourage people to \"build in open\".                                                                   |                                                                                                                                                               |\n",
    "| Further developments in Generative AI and march towards AGI (Artificial General Intelligence).         |                                                                                                                                                               |\n",
    "\n",
    "**Enough talk, let's start hacking!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"GROQ_API_KEY\", \"FIRECRAWL_API_KEY\", \"SERPER_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "setup_env()\n",
    "clear_output()\n",
    "print(\"🚀 Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "DEFAULT_LINKS_CACHE_FILE = \"outputs/links_cache.json\"\n",
    "\n",
    "\n",
    "class LinksCache:\n",
    "    \"\"\"A simple in-memory cache for storing links.\"\"\"\n",
    "\n",
    "    def __init__(self, links_cache_file=DEFAULT_LINKS_CACHE_FILE):\n",
    "        \"\"\"Initializes the links cache.\"\"\"\n",
    "        self.load_links_cache_file = links_cache_file\n",
    "        self.links_cache_file = links_cache_file\n",
    "        self.links_cache = self.__load_links_cache()\n",
    "\n",
    "    def __load_links_cache(self) -> Dict[str, list]:\n",
    "        \"\"\"Loads the links cache from the file\"\"\"\n",
    "        if not os.path.exists(self.links_cache_file):\n",
    "            with open(self.links_cache_file, \"w\") as f:\n",
    "                json.dump({}, f)\n",
    "            self.links_cache = {}\n",
    "            return self.links_cache\n",
    "\n",
    "        with open(self.links_cache_file, \"r\") as f:\n",
    "            self.links_cache = json.load(f)\n",
    "        return self.links_cache\n",
    "\n",
    "    def __save_links_cache(self) -> None:\n",
    "        \"\"\"Saves the links cache to the file\"\"\"\n",
    "        with open(self.links_cache_file, \"w\") as f:\n",
    "            json.dump(self.links_cache, f)\n",
    "\n",
    "    def add_links_to_cache(self, query: str, links: List[str]) -> None:\n",
    "        \"\"\"Adds the links to the cache\"\"\"\n",
    "        self.links_cache[query] = links\n",
    "        self.__dedupe_links_cache()\n",
    "        # self.save_links_cache() # Not required because dedupe links cache already saves the cache.\n",
    "\n",
    "    def has(self, query: str) -> bool:\n",
    "        \"\"\"Checks if the query is in the cache\"\"\"\n",
    "        return query in self.links_cache\n",
    "\n",
    "    def get(self, query: str) -> List[str]:\n",
    "        \"\"\"Gets the links from the cache\"\"\"\n",
    "        return self.links_cache.get(query)\n",
    "\n",
    "    def __dedupe_links_cache(self) -> None:\n",
    "        \"\"\"Dedupes the links cache\"\"\"\n",
    "        for query, links in self.links_cache.items():\n",
    "            self.links_cache[query] = self.__dedupe_links(links)\n",
    "        self.__save_links_cache()\n",
    "\n",
    "    def __dedupe_links(self, links: List[str]) -> List[str]:\n",
    "        \"\"\"Dedupes the links\"\"\"\n",
    "        return list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Utils:\n",
    "    \"\"\"Common utility functions for text processing and analysis.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def count_tokens(text: str) -> int:\n",
    "        \"\"\"\n",
    "        Approximate the number of tokens in a text input for an LLM.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to calculate tokens for.\n",
    "            encoding_model (str): The tokenization model to use (e.g., 'cl100k_base').\n",
    "                                This allows integration with a tokenizer library for better accuracy.\n",
    "\n",
    "        Returns:\n",
    "            int: Approximate number of tokens in the input text.\n",
    "        \"\"\"\n",
    "        # Clean text and normalize spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "        # Approximate tokenization\n",
    "        # 1. Split on spaces, punctuation, and common subword patterns\n",
    "        # 2. Adjust weights based on encoding_model if needed\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "        # Return the token count\n",
    "        return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from typing import List, Any\n",
    "import traceback\n",
    "from pprint import pp\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class SOFiCSearchEngine:\n",
    "    \"\"\"SOFIC (Serper Orchestrated FireCrawl) Search Engine:: A simple search engine built on FireCrawl Python SDK.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Intializes the FireCrawlSearchEngine class.\"\"\"\n",
    "\n",
    "        self.firecrawl = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n",
    "        self.cache = {}\n",
    "        self.links_cache = LinksCache()\n",
    "        self.docs_cache = {}\n",
    "        print(\"🔥 SOFiC Search Engine initialized. 🕷️\")\n",
    "\n",
    "    def __serper_search(self, query: str) -> List[str]:\n",
    "        \"\"\"Searches a search engine, currently Google via Serper API.\"\"\"\n",
    "        search = GoogleSerperAPIWrapper()\n",
    "        response = search.results(query)\n",
    "        return response\n",
    "\n",
    "    def search(self, query: str) -> List[str]:\n",
    "        \"\"\"Searches the query using the search engine.\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        print(f\"📡 Searching for query: {query}\")\n",
    "        results = {}\n",
    "        processed_pages = []\n",
    "        initial_links = self.get_links_for_query(query)\n",
    "        print(f\"Got {len(initial_links)} links for query: {query}\")\n",
    "        for link in initial_links:\n",
    "            print(f\"Getting page content in markdown from URL: {link}\")\n",
    "            result = {\"url\": link}\n",
    "            page_markdown, tokens_fetched = self.get_page_markdown(link)\n",
    "            if page_markdown:\n",
    "                result[\"markdown\"] = page_markdown\n",
    "                result[\"tokens_fetched\"] = tokens_fetched\n",
    "                processed_pages.append(result)\n",
    "        results[\"processed_pages\"] = processed_pages\n",
    "        results[\"total_tokens_fetched\"] = sum(\n",
    "            [page[\"tokens_fetched\"] for page in processed_pages]\n",
    "        )\n",
    "        end_time = datetime.now()\n",
    "        time_difference_seconds = str((end_time - start_time).total_seconds()) + \"s\"\n",
    "        print(f\"🕒 Search completed in {time_difference_seconds}\")\n",
    "        return results\n",
    "\n",
    "    def get_links_for_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Gets the links for a given query.\"\"\"\n",
    "\n",
    "        def collect_links(response: Dict[str, Any]) -> List[str]:\n",
    "            \"\"\"Collects the links from the response\"\"\"\n",
    "            organic = response.get(\"organic\")\n",
    "\n",
    "            if not organic:\n",
    "                print(\"No organic results found.\")\n",
    "                return []\n",
    "\n",
    "            links = []\n",
    "            for item in organic:\n",
    "                sitelinks = item.get(\"sitelinks\")\n",
    "                if sitelinks or (type(sitelinks) == list and len(sitelinks) > 0):\n",
    "                    for link in sitelinks:\n",
    "                        links.append(link[\"link\"])\n",
    "                links.append(item[\"link\"])\n",
    "            return links\n",
    "\n",
    "        if self.links_cache.has(query):\n",
    "            return self.links_cache.get(query)\n",
    "\n",
    "        response = self.__serper_search(query)\n",
    "        links = collect_links(response)\n",
    "        self.links_cache.add_links_to_cache(query, links)\n",
    "        return self.links_cache.get(query)\n",
    "\n",
    "    def get_links(self, input_url: str) -> List[str]:\n",
    "        \"\"\"Gets the links from the given URL.\"\"\"\n",
    "        try:\n",
    "            print(f\"Getting links from URL: {input_url}\")\n",
    "            cache_key = f\"{input_url}_links\"\n",
    "            cached_links = cache.get(cache_key)\n",
    "            if cached_links:\n",
    "                print(f\"Using cached links for URL: {input_url}\")\n",
    "                return cached_links\n",
    "\n",
    "            app = self.firecrawl\n",
    "            crawl_result = app.map_url(input_url)\n",
    "\n",
    "            success = crawl_result[\"success\"]\n",
    "\n",
    "            if not success:\n",
    "                raise RuntimeError(f\"Failed to get links from URL: {input_url}\")\n",
    "\n",
    "            links = crawl_result[\"links\"]\n",
    "            print(f\"Got {len(links)} links from URL: {input_url}\")\n",
    "            cache[cache_key] = links\n",
    "\n",
    "            return links\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get links from URL: {input_url}\")\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "    def get_page_markdown(self, link: str) -> str:\n",
    "        \"\"\"Gets the documentation from the given link.\"\"\"\n",
    "        try:\n",
    "            print(f\"Getting page content in markdown from URL: {link}\")\n",
    "            cached_doc = self.docs_cache.get(link)\n",
    "            if cached_doc:\n",
    "                print(f\"Using cached docs for URL: {link}\")\n",
    "                return cached_doc\n",
    "\n",
    "            app = self.firecrawl\n",
    "            print(f\"Getting page content in markdown from URL: {link}\")\n",
    "            scrape_result = app.scrape_url(link, params={\"formats\": [\"markdown\"]})\n",
    "\n",
    "            if not scrape_result:\n",
    "                return None\n",
    "\n",
    "            success = scrape_result[\"metadata\"][\"statusCode\"] == 200\n",
    "\n",
    "            if not success:\n",
    "                print(f\"Failed to get docs from URL: {link}\")\n",
    "                return None\n",
    "\n",
    "            print(f\"Got page content in markdown from URL: {link}\")\n",
    "\n",
    "            markdown = scrape_result[\"markdown\"]\n",
    "            self.docs_cache[link] = markdown\n",
    "\n",
    "            token_count = Utils.count_tokens(markdown)\n",
    "            tokens_fetched = token_count\n",
    "            print(f\"Token count: {token_count}\")\n",
    "\n",
    "            print(f\"Markdown size: {len(markdown)}\")\n",
    "\n",
    "            return markdown, tokens_fetched\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get page content in markdown from URL: {link}\")\n",
    "            traceback.print_exc()\n",
    "            return \"\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def write_results_to_file(results, query):\n",
    "    \"\"\"Writes the search results to a file.\"\"\"\n",
    "    print(\"Writing search results to file...\")\n",
    "    with open(f\"outputs/search_results_{query}.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    print(\"Search results written to file.\")\n",
    "\n",
    "\n",
    "def search_internal(query: str):\n",
    "    \"\"\"Searches the query using the FireCrawl search engine.\"\"\"\n",
    "    search_engine = SOFiCSearchEngine()\n",
    "    response = search_engine.search(query)\n",
    "    clear_output()\n",
    "    print(f\"Search results for {query} computed.\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_search_engine(query: str):\n",
    "    \"\"\"Runs the FireCrawl search engine with the given query.\"\"\"\n",
    "    response = search_internal(query)\n",
    "    write_results_to_file(response, query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Big Tech Meta Key Highlights 2024\"\n",
    "results = run_search_engine(query=query)\n",
    "clear_output()\n",
    "pp(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
