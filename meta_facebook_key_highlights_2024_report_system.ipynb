{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta (previously Facebook) Key Highlights 2024 Report System [â—ï¸ Work In Progress]\n",
    "\n",
    "> Disclaimer 1 ðŸ’¡: This project is for educational, academic and research purposes. Please use this technology ethically, legally and responsibly.\n",
    "\n",
    "> Disclaimer 2 ðŸ’¡: This project is not officially affiliated with Meta or any of their partner organizations. It's entirely independent and created from ground-up by [AdiPat](https://www.github.com/AdiPat) at [The Hackers Playbook](https://www.thehackersplaybook.substack.com).\n",
    "\n",
    "Welcome! Before we begin, let's set some context. This will help you better understand the underlying motivation and intent of the project. Hopefully, it will motivate and inspire you to improve your programming skills and upgrade your abilities. I've designed this notebook to help you at every step, so if you ever get stuck, spend some time reading each line and folllowing each step exactly as prescribed and you will definitely find a solution. I can't accurately articulate how that works, but it works.\n",
    "\n",
    "## Personal Note\n",
    "\n",
    "- Firstly, this section is subjective. Feel free to ignore it. Whether you choose to believe or disbelieve the contents of this section, in either situation, the outcome of this project is independent of your most likely, well-intentioned judgement.\n",
    "\n",
    "- I've been following Mark Zuckerberg since I was in school, which is approximately since 2009/2010. I don't know the exact year but it was around this time when I learnt about him through the Internet.\n",
    "\n",
    "- We share several similarities in terms of personality, attitude towards life, and most importantly Programming. I don't know him personally so this is based on his public appearances and whatever I have heard or read about him online and from people. Assuming that all the information I gathered is true, then if the world were devoid of biological and cultural nuances, and we were all judged purely as Programmers, it would be fair to say that I'm like Mark Zuckerberg's younger \"programming brother\".\n",
    "\n",
    "- Most importantly, he was and is still a \"hacker\" at heart. If you don't believe me, observe every public announcement Meta makes.\n",
    "\n",
    "- I have never officially worked at Meta but I like to consider myself as an invisible and unofficial contributor (not employee) at Meta. I came up with this idea to motivate myself to keep up to the standards of the ever evolving tech world and to maintain a cultural identity of myself which closely resembles \"The Hacker Culture\".\n",
    "\n",
    "- There's a lot to add, but as time progresses, I'll continue talking about \"Mark Zuckerberg's influence on AdiPat's life\" in greater detail!\n",
    "\n",
    "If you ever have any questions, email us at `thehackersplaybook0@gmail.com`.\n",
    "\n",
    "## Naming Convention\n",
    "\n",
    "The file name and title includes Facebook for backward compatability with search engines and crawlers. Since this research project is Free & Open Source, it's important for the Hacker Culture that it reaches maximum number of people so that humanity can benefit from the efforts and energy directed into this endevour.\n",
    "\n",
    "## Goals & Objectives\n",
    "\n",
    "**Note:** Goals are broad, long-term aspirations that provide direction, while objectives are specific, measurable, and time-bound steps to achieve those goals.\n",
    "\n",
    "| Goals                                                                                                  | Objectives                                                                                                                                                    |\n",
    "| ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| To demonstrate the power of Llama 3.1 in AI-powered automated report generation for enterprises.       | To create a valuable content product for The Hackers Playbook which will help our students upskill.                                                           |\n",
    "| Open a discussion for official partnerships with Meta and their partner organizations.                 | Successfully utilize FireCrawl for information retrieval with measurable markers that can be used by FireCrawl (Mendable) to improve it's product experience. |\n",
    "| Contribute to Meta's Free & Open Source repository of software products. React inspires all of us.     | Contribute to the Python ecosystem. Create an executable experiment for Python Programmers to learn from.                                                     |\n",
    "| Extract key learnings from Meta's 2024 highlights to drive strategic insights at The Hackers Playbook. | Highlight AI-safety, AI-security, Ethical AI and Responsible AI practises in action.                                                                          |\n",
    "| Encourage people to \"build in open\".                                                                   |                                                                                                                                                               |\n",
    "| Further developments in Generative AI and march towards AGI (Artificial General Intelligence).         |                                                                                                                                                               |\n",
    "\n",
    "**Enough talk, let's start hacking!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\n",
    "    \"GROQ_API_KEY\",\n",
    "    \"FIRECRAWL_API_KEY\",\n",
    "    \"SERPER_API_KEY\",\n",
    "    \"GEMINI_API_KEY\",\n",
    "]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "setup_env()\n",
    "clear_output()\n",
    "print(\"ðŸš€ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "DEFAULT_LINKS_CACHE_FILE = \"outputs/links_cache.json\"\n",
    "\n",
    "\n",
    "class LinksCache:\n",
    "    \"\"\"A simple in-memory cache for storing links.\"\"\"\n",
    "\n",
    "    def __init__(self, links_cache_file=DEFAULT_LINKS_CACHE_FILE):\n",
    "        \"\"\"Initializes the links cache.\"\"\"\n",
    "        self.load_links_cache_file = links_cache_file\n",
    "        self.links_cache_file = links_cache_file\n",
    "        self.links_cache = self.__load_links_cache()\n",
    "\n",
    "    def __load_links_cache(self) -> Dict[str, list]:\n",
    "        \"\"\"Loads the links cache from the file\"\"\"\n",
    "        if not os.path.exists(self.links_cache_file):\n",
    "            with open(self.links_cache_file, \"w\") as f:\n",
    "                json.dump({}, f)\n",
    "            self.links_cache = {}\n",
    "            return self.links_cache\n",
    "\n",
    "        with open(self.links_cache_file, \"r\") as f:\n",
    "            self.links_cache = json.load(f)\n",
    "        return self.links_cache\n",
    "\n",
    "    def __save_links_cache(self) -> None:\n",
    "        \"\"\"Saves the links cache to the file\"\"\"\n",
    "        with open(self.links_cache_file, \"w\") as f:\n",
    "            json.dump(self.links_cache, f)\n",
    "\n",
    "    def add_links_to_cache(self, query: str, links: List[str]) -> None:\n",
    "        \"\"\"Adds the links to the cache\"\"\"\n",
    "        self.links_cache[query] = links\n",
    "        self.__dedupe_links_cache()\n",
    "        # self.save_links_cache() # Not required because dedupe links cache already saves the cache.\n",
    "\n",
    "    def has(self, query: str) -> bool:\n",
    "        \"\"\"Checks if the query is in the cache\"\"\"\n",
    "        return query in self.links_cache\n",
    "\n",
    "    def get(self, query: str) -> List[str]:\n",
    "        \"\"\"Gets the links from the cache\"\"\"\n",
    "        return self.links_cache.get(query)\n",
    "\n",
    "    def __dedupe_links_cache(self) -> None:\n",
    "        \"\"\"Dedupes the links cache\"\"\"\n",
    "        for query, links in self.links_cache.items():\n",
    "            self.links_cache[query] = self.__dedupe_links(links)\n",
    "        self.__save_links_cache()\n",
    "\n",
    "    def __dedupe_links(self, links: List[str]) -> List[str]:\n",
    "        \"\"\"Dedupes the links\"\"\"\n",
    "        return list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Utils:\n",
    "    \"\"\"Common utility functions for text processing and analysis.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def count_tokens(text: str) -> int:\n",
    "        \"\"\"\n",
    "        Approximate the number of tokens in a text input for an LLM.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to calculate tokens for.\n",
    "            encoding_model (str): The tokenization model to use (e.g., 'cl100k_base').\n",
    "                                This allows integration with a tokenizer library for better accuracy.\n",
    "\n",
    "        Returns:\n",
    "            int: Approximate number of tokens in the input text.\n",
    "        \"\"\"\n",
    "        # Clean text and normalize spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "        # Approximate tokenization\n",
    "        # 1. Split on spaces, punctuation, and common subword patterns\n",
    "        # 2. Adjust weights based on encoding_model if needed\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "        # Return the token count\n",
    "        return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from typing import List, Any, Union\n",
    "import traceback\n",
    "from pprint import pp\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "DEFAULT_SEARCH_RETRY_COUNT = 3\n",
    "EXPONENTIAL_BACKOFF_FACTOR = 2\n",
    "\n",
    "\n",
    "def get_exponential_backoff_delay(\n",
    "    retry_count: int, backoff_factor=EXPONENTIAL_BACKOFF_FACTOR\n",
    ") -> int:\n",
    "    \"\"\"Gets the exponential backoff delay for retries.\"\"\"\n",
    "    return backoff_factor**retry_count\n",
    "\n",
    "\n",
    "def exponential_backofff_retry(\n",
    "    retry_count: int, backoff_factor=EXPONENTIAL_BACKOFF_FACTOR\n",
    "):\n",
    "    \"\"\"Retries the function using exponential backoff.\"\"\"\n",
    "\n",
    "    def exponential_backoff_decorator(func):\n",
    "        \"\"\"Decorator function for exponential backoff.\"\"\"\n",
    "\n",
    "        def exponential_backoff_wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                error = result.get(\"error\") if type(result) == dict else None\n",
    "\n",
    "                if error and \"page is not supported\" in error:\n",
    "                    print(\"Page is not supported, skipped retry.\")\n",
    "                    return result\n",
    "\n",
    "                if not result or (type(result) == dict and error):\n",
    "                    raise Exception(f\"{func.__name__} failed.\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                error = str(e)\n",
    "                is_page_not_supported = \"This website is no longer supported\" in error\n",
    "                if is_page_not_supported:\n",
    "                    print(\"Page is not supported, skipped retry.\")\n",
    "                    return {\n",
    "                        \"url\": args[1],\n",
    "                        \"markdown\": \"\",\n",
    "                        \"error\": \"This page is not supported by FireCrawl\",\n",
    "                        \"tokens_fetched\": 0,\n",
    "                    }\n",
    "                if retry_count <= 0:\n",
    "                    print(f\"Failed to execute function: {func.__name__}\")\n",
    "                    return None\n",
    "                print(\"Adding exponential backoff delay.\")\n",
    "                delay_time = get_exponential_backoff_delay(\n",
    "                    retry_count, backoff_factor=backoff_factor\n",
    "                )\n",
    "                time.sleep(delay_time)\n",
    "                print(f\"Retrying function: {func.__name__}.\")\n",
    "                return exponential_backoff_wrapper(*args, **kwargs)\n",
    "\n",
    "        return exponential_backoff_wrapper\n",
    "\n",
    "    return exponential_backoff_decorator\n",
    "\n",
    "\n",
    "class MARCSearchEngine:\n",
    "    \"\"\"MARC (Machine Automated Retrieval & Curation) Search Engine:: A simple search engine built on FireCrawl Python SDK.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Intializes the FireCrawlSearchEngine class.\"\"\"\n",
    "\n",
    "        self.firecrawl = FirecrawlApp(api_key=os.environ.get(\"FIRECRAWL_API_KEY\"))\n",
    "        self.cache = {}\n",
    "        self.links_cache = LinksCache()\n",
    "        self.docs_cache = {}\n",
    "        print(\"ðŸ”¥ MARC Search Engine initialized. ðŸ•·ï¸\")\n",
    "\n",
    "    @exponential_backofff_retry(DEFAULT_SEARCH_RETRY_COUNT)\n",
    "    def __serper_search(self, query: str) -> Union[Dict[str, Any], None]:\n",
    "        \"\"\"Searches a search engine, currently Google via Serper API.\"\"\"\n",
    "        try:\n",
    "            print(f\"Running Serper search for query: {query}\")\n",
    "            search = GoogleSerperAPIWrapper()\n",
    "            response = search.results(query)\n",
    "            print(f\"Got response for query: {query}\", response)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to search for query: {query}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def search(self, query: str) -> List[str]:\n",
    "        \"\"\"Searches the query using the search engine.\"\"\"\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            print(f\"ðŸ“¡ Searching for query: {query}\")\n",
    "            results = {}\n",
    "            processed_pages = []\n",
    "            initial_links = self.get_links_for_query(query)\n",
    "            print(f\"Got {len(initial_links)} links for query: {query}\")\n",
    "            for link in initial_links:\n",
    "                try:\n",
    "                    print(f\"Getting page content in markdown from URL: {link}\")\n",
    "                    result = {\"url\": link}\n",
    "                    page_crawl_result = self.__get_page_markdown(link)\n",
    "                    page_markdown = page_crawl_result.get(\"markdown\")\n",
    "                    tokens_fetched = page_crawl_result.get(\"tokens_fetched\")\n",
    "                    error = page_crawl_result.get(\"error\")\n",
    "                    result[\"markdown\"] = page_markdown\n",
    "                    result[\"tokens_fetched\"] = tokens_fetched\n",
    "                    result[\"error\"] = error\n",
    "                    processed_pages.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to get page content in markdown from URL: {link}\")\n",
    "                    traceback.print_exc()\n",
    "                    result = {\n",
    "                        \"url\": link,\n",
    "                        \"markdown\": \"\",\n",
    "                        \"error\": str(e),\n",
    "                        \"tokens_fetched\": 0,\n",
    "                    }\n",
    "                    processed_pages.append(result)\n",
    "            results[\"processed_pages\"] = processed_pages\n",
    "            results[\"total_tokens_fetched\"] = sum(\n",
    "                [page[\"tokens_fetched\"] for page in processed_pages]\n",
    "            )\n",
    "            end_time = datetime.now()\n",
    "            time_difference_seconds = str((end_time - start_time).total_seconds()) + \"s\"\n",
    "            print(f\"ðŸ•’ Search completed in {time_difference_seconds}\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed with unexpected error: {query}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_links_for_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Gets the links for a given query.\"\"\"\n",
    "\n",
    "        def collect_links(response: Dict[str, Any]) -> List[str]:\n",
    "            \"\"\"Collects the links from the response\"\"\"\n",
    "            organic = response.get(\"organic\")\n",
    "\n",
    "            if not organic:\n",
    "                print(\"No organic results found.\")\n",
    "                return []\n",
    "\n",
    "            links = []\n",
    "            for item in organic:\n",
    "                sitelinks = item.get(\"sitelinks\")\n",
    "                if sitelinks or (type(sitelinks) == list and len(sitelinks) > 0):\n",
    "                    for link in sitelinks:\n",
    "                        links.append(link[\"link\"])\n",
    "                links.append(item[\"link\"])\n",
    "            return links\n",
    "\n",
    "        if self.links_cache.has(query):\n",
    "            print(f\"Using cached links for query: {query}\")\n",
    "            return self.links_cache.get(query)\n",
    "\n",
    "        response = self.__serper_search(query)\n",
    "        if not response:\n",
    "            print(f\"Failed to get links for query: {query}\")\n",
    "            return []\n",
    "        links = collect_links(response)\n",
    "        self.links_cache.add_links_to_cache(query, links)\n",
    "        return self.links_cache.get(query)\n",
    "\n",
    "    def get_links(self, input_url: str) -> List[str]:\n",
    "        \"\"\"Gets the links from the given URL.\"\"\"\n",
    "        try:\n",
    "            print(f\"Getting links from URL: {input_url}\")\n",
    "            cache_key = f\"{input_url}_links\"\n",
    "            cached_links = self.cache.get(cache_key)\n",
    "            if cached_links:\n",
    "                print(f\"Using cached links for URL: {input_url}\")\n",
    "                return cached_links\n",
    "\n",
    "            app = self.firecrawl\n",
    "            crawl_result = app.map_url(input_url)\n",
    "\n",
    "            success = crawl_result[\"success\"]\n",
    "\n",
    "            if not success:\n",
    "                raise RuntimeError(f\"Failed to get links from URL: {input_url}\")\n",
    "\n",
    "            links = crawl_result[\"links\"]\n",
    "            print(f\"Got {len(links)} links from URL: {input_url}\")\n",
    "            self.cache[cache_key] = links\n",
    "\n",
    "            return links\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get links from URL: {input_url}\")\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "    @exponential_backofff_retry(DEFAULT_SEARCH_RETRY_COUNT)\n",
    "    def __get_page_markdown(self, link: str) -> Any:\n",
    "        \"\"\"Gets the documentation from the given link.\"\"\"\n",
    "        try:\n",
    "            print(f\"Getting page content in markdown from URL: {link}\")\n",
    "            cached_doc = self.docs_cache.get(link)\n",
    "            if cached_doc:\n",
    "                print(f\"Using cached docs for URL: {link}\")\n",
    "                return {\n",
    "                    \"url\": link,\n",
    "                    \"markdown\": cached_doc,\n",
    "                    \"error\": \"Fetching markdown page failed due no response.\",\n",
    "                    \"tokens_fetched\": 0,\n",
    "                }\n",
    "\n",
    "            app = self.firecrawl\n",
    "            print(f\"Getting page content in markdown from URL: {link}\")\n",
    "            scrape_result = app.scrape_url(link, params={\"formats\": [\"markdown\"]})\n",
    "\n",
    "            if not scrape_result:\n",
    "                return {\n",
    "                    \"url\": link,\n",
    "                    \"markdown\": \"\",\n",
    "                    \"error\": \"Fetching markdown page failed due no response.\",\n",
    "                    \"tokens_fetched\": 0,\n",
    "                }\n",
    "\n",
    "            success = scrape_result[\"metadata\"][\"statusCode\"] == 200\n",
    "\n",
    "            if not success:\n",
    "                print(f\"Failed to get docs from URL: {link}\")\n",
    "                return {\n",
    "                    \"url\": link,\n",
    "                    \"markdown\": \"\",\n",
    "                    \"error\": \"Fetching markdown page failed due failed success flag.\",\n",
    "                    \"tokens_fetched\": 0,\n",
    "                }\n",
    "\n",
    "            print(f\"Got page content in markdown from URL: {link}\")\n",
    "\n",
    "            markdown = scrape_result.get(\"markdown\")\n",
    "\n",
    "            if not markdown:\n",
    "                print(f\"No markdown found in scrape result.\")\n",
    "                return {\n",
    "                    \"url\": link,\n",
    "                    \"markdown\": \"\",\n",
    "                    \"error\": \"Fetching markdown page failed due to no markdown.\",\n",
    "                    \"tokens_fetched\": 0,\n",
    "                }\n",
    "            self.docs_cache[link] = markdown\n",
    "\n",
    "            token_count = Utils.count_tokens(markdown)\n",
    "            tokens_fetched = token_count\n",
    "            print(f\"Token count: {token_count}\")\n",
    "\n",
    "            print(f\"Markdown size: {len(markdown)}\")\n",
    "\n",
    "            return {\n",
    "                \"url\": link,\n",
    "                \"markdown\": markdown,\n",
    "                \"error\": \"\",\n",
    "                \"tokens_fetched\": tokens_fetched,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            is_page_not_supported = \"This website is no longer supported\" in str(e)\n",
    "            print(f\"Failed to get page content in markdown from URL: {link}\")\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"url\": link,\n",
    "                \"markdown\": \"\",\n",
    "                \"error\": (\n",
    "                    \"This page is not supported by FireCrawl\"\n",
    "                    if is_page_not_supported\n",
    "                    else str(e)\n",
    "                ),\n",
    "                \"tokens_fetched\": 0,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "CACHED_FILE_THRESHOLD_SIZE = 2048\n",
    "\n",
    "\n",
    "def ensure_file_and_directory_exists(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Ensures that the directory and file at the given path exist.\n",
    "    If the directory does not exist, it is created recursively.\n",
    "    If the file does not exist, an empty file is created.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to check and create.\n",
    "    \"\"\"\n",
    "    # Extract the directory from the file path\n",
    "    directory = os.path.dirname(file_path)\n",
    "\n",
    "    # Check if the directory exists, if not, create it recursively\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory created: {directory}\")\n",
    "\n",
    "    # Check if the file exists, if not, create an empty file\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as file:\n",
    "            json.dump({}, file)  # Create an empty file\n",
    "        print(f\"File created: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File already exists: {file_path}\")\n",
    "\n",
    "\n",
    "def query_to_alphanumeric(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a query string into an alphanumeric string with underscores.\n",
    "    Non-alphanumeric characters are replaced with underscores.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The sanitized alphanumeric string with underscores.\n",
    "    \"\"\"\n",
    "    # Replace non-alphanumeric characters with underscores\n",
    "    sanitized = re.sub(r\"\\W+\", \"_\", query)\n",
    "    # Remove leading or trailing underscores\n",
    "    sanitized = sanitized.strip(\"_\")\n",
    "    return sanitized\n",
    "\n",
    "\n",
    "def get_query_output_file_path(query: str) -> str:\n",
    "    \"\"\"Gets the output file path for the query.\"\"\"\n",
    "    file_name = query_to_alphanumeric(query)\n",
    "    return f\"outputs/search_results_{file_name}.json\"\n",
    "\n",
    "\n",
    "def write_results_to_file(results, query) -> None:\n",
    "    \"\"\"Writes the search results to a file.\"\"\"\n",
    "    print(\"Writing search results to file...\")\n",
    "    ensure_file_and_directory_exists(get_query_output_file_path(query))\n",
    "    file_path = get_query_output_file_path(query)\n",
    "    does_file_exist = False\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            does_file_exist = True\n",
    "    except FileNotFoundError:\n",
    "        does_file_exist = False\n",
    "    if not does_file_exist:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            pass  # Do nothing, just ensure the file is created\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    print(\"Search results written to file.\")\n",
    "\n",
    "\n",
    "def read_results_from_file(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Reads the search results from the file.\"\"\"\n",
    "    try:\n",
    "        ensure_file_and_directory_exists(get_query_output_file_path(query))\n",
    "        file_path = get_query_output_file_path(query)\n",
    "        does_file_exist = False\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                does_file_exist = True\n",
    "        except FileNotFoundError:\n",
    "            does_file_exist = False\n",
    "        if not does_file_exist:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                pass  # Do nothing, just ensure the file is created\n",
    "            with open(file_path, \"w\") as f:\n",
    "                json.dump({}, f)\n",
    "            return {}\n",
    "        with open(file_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read search results from file: {query}\")\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "\n",
    "def is_search_results_cached(query: str) -> bool:\n",
    "    \"\"\"Checks if the search results are cached.\"\"\"\n",
    "    content = read_results_from_file(query)\n",
    "    content_json = json.dumps(content)\n",
    "    return content is not None and len(content_json) > CACHED_FILE_THRESHOLD_SIZE\n",
    "\n",
    "\n",
    "def search_internal(query: str) -> Any:\n",
    "    \"\"\"Searches the query using the FireCrawl search engine.\"\"\"\n",
    "    if is_search_results_cached(query):\n",
    "        print(f\"Search results for {query} are cached.\")\n",
    "        return read_results_from_file(query)\n",
    "    search_engine = MARCSearchEngine()\n",
    "    response = search_engine.search(query)\n",
    "    print(f\"Search results for {query} computed.\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_search_engine(query: str, write_output=True) -> Any:\n",
    "    \"\"\"Runs the FireCrawl search engine with the given query.\"\"\"\n",
    "    print(f\"Running search engine for query: {query}\")\n",
    "    response = search_internal(query)\n",
    "    print(\"Search engine completed.\")\n",
    "    if write_output:\n",
    "        print(\"!!! Writing search results to file.\")\n",
    "        write_results_to_file(response, query)\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_search_engine_batch(queries: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Runs the FireCrawl search engine with the given queries.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    batch_query = f\"meta_report_batch_{start_time.timestamp()}\"\n",
    "    existing_results = read_results_from_file(batch_query)\n",
    "    print(f\"Running search engine in batch mode for {len(queries)} queries!\")\n",
    "    results = {}\n",
    "    for query in queries:\n",
    "        cached_query = existing_results.get(query)\n",
    "        if (\n",
    "            cached_query\n",
    "            and cached_query.get(\"processed_pages\")\n",
    "            and len(cached_query.get(\"processed_pages\")) > 0\n",
    "        ):\n",
    "            print(f\"Skipping query: {query} as it is already cached.\")\n",
    "            results[query] = existing_results[query]\n",
    "            continue  # Skip if the query is already cached\n",
    "        print(f\"Running search engine for query: {query}\")\n",
    "        results[query] = run_search_engine(query, write_output=False)\n",
    "        # results[query] = {}\n",
    "        print(f\"Search engine completed for query: {query}\")\n",
    "        print(\"Writing search results to file...\")\n",
    "        write_results_to_file(results, batch_query)\n",
    "        print(\"Search results written to file.\")\n",
    "    write_results_to_file(results, batch_query)\n",
    "    end_time = datetime.now()\n",
    "    time_difference_seconds = str((end_time - start_time).total_seconds()) + \"s\"\n",
    "    print(f\"Batch search engine completed in {time_difference_seconds}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Meta's Performance in AR/VR and Metaverse Projects in 2024\"\n",
    "results = run_search_engine(query=query)\n",
    "pp(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Big Tech Meta Key Highlights 2024\",\n",
    "    \"Meta 2024 News\",\n",
    "    \"Meta 2024 Updates\",\n",
    "    \"Meta 2024 Business Model Changes and Revenue Streams\",\n",
    "    \"Meta Hiring Trends and Job Market Analysis 2024\",\n",
    "    \"Meta Key Strategic Decisions and Initiatives 2024\",\n",
    "    \"Meta's Approach to AI and Emerging Technologies in 2024\",\n",
    "    \"Meta Quarterly Financial Reports 2024: Analysis and Insights\",\n",
    "    \"Meta's Investments and Acquisitions in 2024\",\n",
    "    \"Meta's Impact on the Social Media Ecosystem in 2024\",\n",
    "    \"Meta's Policy and Compliance Updates 2024: User Privacy and Regulation\",\n",
    "    \"Meta's Performance in AR/VR and Metaverse Projects in 2024\",\n",
    "    \"Meta's Global Expansion and Regional Strategies in 2024\",\n",
    "]\n",
    "\n",
    "results = run_search_engine_batch(queries=queries)\n",
    "pp(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupported URL\n",
    "\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key=os.environ.get(\"FIRECRAW_API_KEY\"))\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=1lr0Fi4ZP34\"\n",
    "try:\n",
    "    response = app.scrape_url(url, params={\"formats\": [\"markdown\"]})\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    print(f\"Failed to scrape URL: {url}\")\n",
    "    if \"This website is no longer supported\" in error_message:\n",
    "        print(\"Error: This website is no longer supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "DEFAULT_GEMINI_MODEL = \"gemini-1.5-flash\"\n",
    "\n",
    "\n",
    "def get_gemini_client(model=DEFAULT_GEMINI_MODEL):\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel(model_name=model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def gemini(query, model=DEFAULT_GEMINI_MODEL):\n",
    "    model = get_gemini_client(model)\n",
    "    response = model.generate_content(query)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Markdown, display\n",
    "\n",
    "\n",
    "def read_data_from_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Reads the data from the given file path.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_markdown(file_path: str):\n",
    "    data = read_data_from_file(file_path)\n",
    "    titles = data.keys()\n",
    "    result_markdown = \"\"\n",
    "    for title in titles:\n",
    "        title_markdown = f\"# {title}\\n\\n\"\n",
    "        body_markdown = \"\"\n",
    "        processed_pages = data[title].get(\"processed_pages\")\n",
    "        cur_page_count = 0\n",
    "        if processed_pages:\n",
    "            print(f\"Processing {len(processed_pages)} pages for {title}.\")\n",
    "            body_markdown = \"\\n\\n\".join([page[\"markdown\"] for page in processed_pages])\n",
    "            cur_page_count += len(processed_pages)\n",
    "            print(f\"Processed pages for {title}: {cur_page_count}\")\n",
    "            result_markdown += title_markdown + body_markdown + \"\\n----\\n\"\n",
    "        else:\n",
    "            print(f\"No processed pages found for {title}.\")\n",
    "    return result_markdown\n",
    "\n",
    "\n",
    "def show_raw_report(file_path: str) -> None:\n",
    "    markdown = extract_markdown(file_path)\n",
    "    clear_output()\n",
    "    display(Markdown(markdown))\n",
    "\n",
    "\n",
    "def show_cleaned_report(\n",
    "    file_path: str, output_file_path: str = \"outputs/meta_key_highlights_2024.md\"\n",
    ") -> None:\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Generating cleaned report from raw data at: {file_path}\")\n",
    "    markdown = extract_markdown(file_path)\n",
    "    prompt = f\"\"\"\n",
    "        Generate a cleaned report titled \"Meta Key Highlights 2024\" from the raw report data.\n",
    "\n",
    "        - Add an Introduction and Conclusion in addition to the sections.\n",
    "        - Number each page. It should be a 5 page report.\n",
    "        - Each page should have atleast 4 sections. \n",
    "        - Each section should have atleast 2 paragraphs or 10 bullet points + 1 paragraph introduction.\n",
    "        - Each section should have a title.\n",
    "        - This report is for enterprise use, so make sure the language is technical and professional. \n",
    "        - Don't hallucinate or make up factual information.\n",
    "        - Add a 'References' section at the end with links to the original content. Include atleast 10 references.\n",
    "\n",
    "        Raw Report: '''{markdown}'''\n",
    "    \"\"\"\n",
    "    print(\"Generating cleaned report.\")\n",
    "    cleaned_report = gemini(prompt)\n",
    "    print(\"Generated cleaned report.\")\n",
    "    print(\"Writng cleaned report to file.\")\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        f.write(cleaned_report)\n",
    "    print(f\"Cleaned report written to file: {output_file_path}\")\n",
    "    clear_output()\n",
    "    end_time = datetime.now()\n",
    "    time_difference_seconds = str((end_time - start_time).total_seconds()) + \"s\"\n",
    "    print(f\"Generated cleaned report in {time_difference_seconds}.\")\n",
    "    display(Markdown(cleaned_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"data/meta_report/raw_report_dump_v1.json\"\n",
    "\n",
    "show_raw_report(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_file_path = \"data/meta_report/raw_report_dump_v1.json\"\n",
    "\n",
    "timestamp = datetime.now().timestamp()\n",
    "\n",
    "output_file_path = f\"outputs/meta_key_highlights_2024_{timestamp}.md\"\n",
    "\n",
    "show_cleaned_report(data_file_path, output_file_path=output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
