{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Assistant with Pydantic and Ollama for Dynamic JSON-based Responses  \n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Description  \n",
    "\n",
    "- This application demonstrates an interactive assistant capable of generating structured responses using **Pydantic models** and **Ollama's language model**.  \n",
    "\n",
    "- It showcases how to validate, process, and generate JSON objects based on user-defined schemas while integrating error handling and a structured environment setup.  \n",
    "\n",
    "\n",
    "\n",
    "## Uses  \n",
    "\n",
    "- **Validating JSON structures** for various use cases, such as API responses.  \n",
    "\n",
    "- **Building intelligent assistants** with predefined response formats.  \n",
    "\n",
    "- **Creating reproducible workflows** in Jupyter Notebooks.  \n",
    "\n",
    "- **Simplifying interactions** with language models through schema-driven prompts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Boilerplate Setup\n",
    "This step sets up the boilerplate code for the project by importing necessary libraries (`os`, `load_dotenv`, `clear_output`), initializing global variables such as `requirements_installed` and `REQUIRED_ENV_VARS`, and defining functions for installing dependencies (`install_requirements`) and validating environment variables (`setup_env`). It ensures dependencies are installed from `requirements.txt`, environment variables are verified, and everything is configured correctly before clearing the output and confirming setup completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"üöÄ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Adding a Simple Tool for Arithmetic and Using Ollama Chat for Model Integration  \n",
    "\n",
    "The code defines a function `add_two_numbers` to add two integers and returns the sum. \n",
    "\n",
    "It then uses the `ollama.chat` function with a language model (`llama3.1`) to ask for a sum (e.g., \"What is 10 + 10?\"). \n",
    "\n",
    "If the model needs to use the `add_two_numbers` function, it makes a tool call, and the code checks for this tool call, extracts the tool name and arguments, and prints the results. \n",
    "\n",
    "Finally, it displays the entire response, including the tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two numbers\n",
    "\n",
    "    Args:\n",
    "      a: The first integer number\n",
    "      b: The second integer number\n",
    "\n",
    "    Returns:\n",
    "      int: The sum of the two numbers\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "    \"llama3.1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 10 + 10?\"}],\n",
    "    tools=[add_two_numbers],  # Actual function reference\n",
    ")\n",
    "\n",
    "\n",
    "if response.message.tool_calls:\n",
    "    name = response.message.tool_calls[0].tool_name\n",
    "    print(f\"ü¶ô The tool name is {name}\")\n",
    "    args = response.message.tool_calls[0].args\n",
    "\n",
    "    if args:\n",
    "        print(f\"ü¶ô The arguments are {args}\")\n",
    "\n",
    "\n",
    "print(response, response.message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Listing Available Ollama Models\n",
    "\n",
    "This block retrieves and displays all the available language models provided by Ollama. It ensures the required models are accessible before proceeding with further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Listing available models...\")\n",
    "\n",
    "pretty_formatted_list = list(ollama.list())\n",
    "models = pretty_formatted_list[0][0]\n",
    "print(pretty_formatted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: AI-Powered Structured Response Generation with Pydantic Validation\n",
    "\n",
    "The `build_dummy_pydantic_object` function creates default objects based on a predefined schema. The `generate_object` function sends the prompt to the AI model and returns the response, which is validated using the provided Pydantic schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an intelligent assistant. You are helping the user with their query.\"\n",
    ")\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 100\n",
    "DEFAULT_OLLAMA_MODEL = \"phi4\"\n",
    "DEFAULT_VERBOSE = True\n",
    "DEFAULT_DEBUG = True\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "def generate_object(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=DEFAULT_SYSTEM_PROMPT,\n",
    "    model=DEFAULT_OLLAMA_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    debug=DEFAULT_DEBUG,\n",
    "    verbose=DEFAULT_VERBOSE,\n",
    ") -> Union[BaseModel, None]:\n",
    "    \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "    try:\n",
    "        if verbose or debug:\n",
    "            print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "        prompt_with_structured_output = f\"\"\"\n",
    "            Prompt: {prompt} \n",
    "            SCHEMA: {build_dummy_pydantic_object(response_model).model_dump_json()}\n",
    "            RESPOND IN JSON FORMAT\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            params = {\n",
    "                \"prompt\": prompt_with_structured_output,\n",
    "                \"system\": system,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "            }\n",
    "            params = json.dumps(params, indent=2)\n",
    "            print(f\"Params: {params}\")\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "        response_json = response.message.content  # Get the response content\n",
    "        print(response_json)\n",
    "        response_obj = json.loads(response_json)\n",
    "        response_structured = response_model.model_validate(response_obj)\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(\"Object generated successfully. üéâ\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"EasyLLM Response: {response_json}\")\n",
    "        return response_structured\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Validating JSON Using Pydantic Models\n",
    "\n",
    "This block uses Pydantic to define a schema for a joke object, including a joke and its author. \n",
    "\n",
    "The assistant is prompted to generate a joke that adheres to this schema. The structured JSON response is validated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = (\n",
    "        \"Why did the scarecrow win an award? Because he was outstanding in his field.\"\n",
    "    )\n",
    "    author: str = \"Unknown\"\n",
    "\n",
    "\n",
    "response = generate_object(\"Tell me a joke\", response_model=Joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Requesting and Validating AI-Generated Joke with Pydantic Schema\n",
    "\n",
    "This step generates a joke using an AI model and validates the response with a Pydantic schema. \n",
    "\n",
    "The schema defines the joke‚Äôs structure, including the joke text and author. The AI is prompted to return a joke in this format, and the response is validated. \n",
    "\n",
    "This ensures that the joke is structured and cleanly formatted for easy use in applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "    author: str\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema: {Joke.model_json_schema()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "\"\"\"\n",
    "\n",
    "response = generate_object(prompt, response_model=Joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: AI-Generated Joke with Schema Validation\n",
    "\n",
    "This step generates a joke using an AI model and validates it with a Pydantic schema. \n",
    "\n",
    "The schema defines the joke's structure, including the joke text and author. \n",
    "\n",
    "The AI is asked to respond in a strict JSON format based on the schema. The response is validated and returned in a clean format for easy use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = \"A joke\"\n",
    "    author: str = \"An author\"\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema ‚Äî Example object: {build_dummy_pydantic_object(Joke).model_dump_json()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "Respond strictly in the given schema format.\n",
    "Respond only with valid JSON and don't include anything else.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook effectively demonstrates how to integrate structured data processing with language models. The approach ensures schema validation, making it suitable for applications like API testing, structured data generation, and schema-driven conversations. The use of Pydantic ensures type safety and consistency, while Ollama's models bring the power of conversational AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
