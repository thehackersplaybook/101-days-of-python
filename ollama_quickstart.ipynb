{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Assistant with Pydantic and Ollama for Dynamic JSON-based Responses  \n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Description  \n",
    "\n",
    "- This application demonstrates an interactive assistant capable of generating structured responses using **Pydantic models** and **Ollama's language model**.  \n",
    "\n",
    "- It showcases how to validate, process, and generate JSON objects based on user-defined schemas while integrating error handling and a structured environment setup.  \n",
    "\n",
    "---\n",
    "\n",
    "## Uses  \n",
    "\n",
    "- **Validating JSON structures** for various use cases, such as API responses.  \n",
    "\n",
    "- **Building intelligent assistants** with predefined response formats.  \n",
    "\n",
    "- **Creating reproducible workflows** in Jupyter Notebooks.  \n",
    "\n",
    "- **Simplifying interactions** with language models through schema-driven prompts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Boilerplate Setup\n",
    "\n",
    "This step sets up the boilerplate code for the project. It includes:\n",
    "\n",
    "- ### Import Statements:\n",
    "\n",
    "    - `os`: Used for running shell commands and accessing environment variables.\n",
    "\n",
    "    - `load_dotenv`: Loads environment variables from a `.env` file into the system for easy configuration.\n",
    "\n",
    "    - `clear_output`: Clears the notebook's output to keep it clean after successful setup.\n",
    "\n",
    "\n",
    "\n",
    "- ### Global Variables:\n",
    "\n",
    "    - `requirements_installed`: Tracks whether dependencies are already installed.\n",
    "    \n",
    "    - `max_retries`: Limits how many times the code will retry installing dependencies in case of failure.\n",
    "    \n",
    "    - `REQUIRED_ENV_VARS`: Specifies the environment variables that must be present.\n",
    "\n",
    "\n",
    "\n",
    "- ### `install_requirements` Function:\n",
    "\n",
    "    - Uses the `os.system` command to run `pip install -r requirements.txt`.\n",
    "\n",
    "    - If the installation fails, it retries up to `max_retries`.\n",
    "\n",
    "    - If retries are exhausted, it exits the program with a failure code.\n",
    "\n",
    "\n",
    "\n",
    "- ### `setup_env` Function:\n",
    "\n",
    "    - Loads environment variables from `.env`.\n",
    "\n",
    "    - Verifies the presence of each required variable using the `check_env` function.\n",
    "\n",
    "    - Exits the program if any required environment variable is missing.\n",
    "\n",
    "\n",
    "\n",
    "- ### Execution:\n",
    "\n",
    "    - Calls `install_requirements` to install dependencies.\n",
    "\n",
    "    - Calls `setup_env` to validate the environment.\n",
    "\n",
    "    - Clears the output and confirms the setup is complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"üöÄ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Adding a Simple Tool for Arithmetic and Using Ollama Chat for Model Integration  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Function Definition: **`add_two_numbers`**  \n",
    "\n",
    "#### **Definition**  \n",
    "\n",
    "- **`def add_two_numbers(a: int, b: int) -> int:`**  \n",
    "\n",
    "  - Defines a function named **`add_two_numbers`** that takes two arguments, **`a`** and **`b`**.  \n",
    "\n",
    "  - Both arguments are annotated as integers (**`int`**), and the return type is also expected to be an integer.  \n",
    "\n",
    "#### **Type Annotations**  \n",
    "\n",
    "- **Purpose**: Improves code readability and assists static type-checking tools.  \n",
    "\n",
    "#### **Docstring**  \n",
    "\n",
    "- A multi-line string explaining the function's purpose, arguments, and return value:  \n",
    "\n",
    "  - **Purpose**: Adds two numbers.  \n",
    "\n",
    "  - **Arguments**:  \n",
    "\n",
    "    - **`a`**: First integer to be added.  \n",
    "\n",
    "    - **`b`**: Second integer to be added.  \n",
    "\n",
    "  - **Returns**: The sum of **`a`** and **`b`** as an integer.  \n",
    "\n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`return a + b`**: Performs the arithmetic addition of the two input integers (**`a`** and **`b`**) and returns the result.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Calling the Ollama Chat Function  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`response = ollama.chat(...)`**: Calls the **`ollama.chat`** function to interact with the **`llama3.1`** language model.  \n",
    "\n",
    "#### **Language Model Version**  \n",
    "\n",
    "- **`\"llama3.1\"`**: Specifies the version of the language model to use for the interaction.  \n",
    "\n",
    "#### **Input Message**  \n",
    "\n",
    "- **`messages=[{\"role\": \"user\", \"content\": \"What is 10 + 10?\"}]`**  \n",
    "\n",
    "  - **`messages`**: A list of dictionaries representing a conversation.  \n",
    "\n",
    "  - **`\"role\": \"user\"`**: Indicates the message comes from the user.  \n",
    "\n",
    "  - **`\"content\": \"What is 10 + 10?\"`**: The user‚Äôs query asking the model to compute the sum of 10 and 10.  \n",
    "\n",
    "\n",
    "#### **Available Tools**  \n",
    "\n",
    "- **`tools=[add_two_numbers]`**  \n",
    "\n",
    "  - A list of tools (functions) available to the model during the interaction.  \n",
    "  \n",
    "  - Includes the **`add_two_numbers`** function as a callable tool.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Checking for Tool Calls  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`if response.message.tool_calls:`**  \n",
    "\n",
    "  - Checks if the model has made any tool calls in its response.  \n",
    "\n",
    "#### **Tool Call Details**  \n",
    "\n",
    "- **`name = response.message.tool_calls[0].tool_name`**  \n",
    "\n",
    "  - Extracts the **`tool_name`** of the first tool call in the list.  \n",
    "\n",
    "#### **Output**  \n",
    "\n",
    "- **`print(f\"ü¶ô The tool name is {name}\")`**: Prints the tool name invoked by the model.  \n",
    "\n",
    "#### **Arguments Extraction**  \n",
    "\n",
    "- **`args = response.message.tool_calls[0].args`**  \n",
    "\n",
    "  - Extracts the arguments passed to the tool during the interaction.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Printing the Tool Arguments  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`if args:`**: Checks if arguments were included in the tool call.  \n",
    "\n",
    "#### **Output**  \n",
    "\n",
    "- **`print(f\"ü¶ô The arguments are {args}\")`**: Prints the arguments for the invoked tool.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Printing the Final Response  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`print(response)`**: Displays the entire response object returned by **`ollama.chat`**.  \n",
    "\n",
    "- **`print(response.message.tool_calls)`**: Prints the tool calls made during the interaction for detailed insight.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Summary\n",
    "- The code demonstrates an integration between a language model (llama3.1) and a custom function (add_two_numbers) to handle queries requiring arithmetic operations.\n",
    "\n",
    "- The model intelligently invokes the tool when necessary, passing the required arguments, and the outputs are displayed for validation.\n",
    "\n",
    "- The approach combines the power of conversational AI with tool-assisted operations, enabling dynamic and interactive solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two numbers\n",
    "\n",
    "    Args:\n",
    "      a: The first integer number\n",
    "      b: The second integer number\n",
    "\n",
    "    Returns:\n",
    "      int: The sum of the two numbers\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "    \"llama3.1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 10 + 10?\"}],\n",
    "    tools=[add_two_numbers],  # Actual function reference\n",
    ")\n",
    "\n",
    "\n",
    "if response.message.tool_calls:\n",
    "    name = response.message.tool_calls[0].tool_name\n",
    "    print(f\"ü¶ô The tool name is {name}\")\n",
    "    args = response.message.tool_calls[0].args\n",
    "\n",
    "    if args:\n",
    "        print(f\"ü¶ô The arguments are {args}\")\n",
    "\n",
    "\n",
    "print(response, response.message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Listing Available Ollama Models\n",
    "\n",
    "This block retrieves and displays all the available language models provided by Ollama. It ensures the required models are accessible before proceeding with further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Listing available models...\")\n",
    "\n",
    "pretty_formatted_list = list(ollama.list())\n",
    "models = pretty_formatted_list[0][0]\n",
    "print(pretty_formatted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: AI-Powered Structured Response Generation with Pydantic Validation\n",
    "\n",
    "\n",
    "### 1. Setting Default Parameters and Imports  \n",
    "\n",
    "#### **Imports**  \n",
    "\n",
    "- **`from typing import Union`**  \n",
    "\n",
    "  - Imports the **`Union`** type hint, allowing functions to specify multiple possible return types (e.g., a function may return a specific object or **`None`**).  \n",
    "\n",
    "- **`from pydantic import BaseModel`**  \n",
    "\n",
    "  - Imports **`BaseModel`** from **`Pydantic`**, a Python library for data validation and settings management. \n",
    "\n",
    "  - **`BaseModel`** is used to define structured data schemas.  \n",
    "\n",
    "- **`import json`**  \n",
    "\n",
    "  - Imports the **`json`** library for handling **JSON (JavaScript Object Notation)** data, widely used in APIs.  \n",
    "\n",
    "- **`import traceback`**  \n",
    "\n",
    "  - Imports **`traceback`** to print detailed error messages and debug stack traces when exceptions occur.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Defining Constants  \n",
    "\n",
    "#### **Constants**  \n",
    "\n",
    "- **`DEFAULT_SYSTEM_PROMPT`**  \n",
    "\n",
    "  - A string constant containing a predefined system instruction for the AI model.  \n",
    "\n",
    "  - Ensures the model remains helpful and consistent in its responses.  \n",
    "\n",
    "- **`DEFAULT_TEMPERATURE`**  \n",
    "\n",
    "  - Controls randomness in AI responses.  \n",
    "\n",
    "  - A lower value like 0.5 produces more focused and deterministic outputs.  \n",
    "\n",
    "- **`DEFAULT_MAX_TOKENS`**  \n",
    "\n",
    "  - Specifies the maximum number of tokens (words or parts of words) the model can generate in its response.  \n",
    "\n",
    "- **`DEFAULT_OLLAMA_MODEL`**  \n",
    "\n",
    "  - Sets the default model to **`phi4`** for handling requests.  \n",
    "\n",
    "- **`DEFAULT_VERBOSE` and `DEFAULT_DEBUG`**  \n",
    "\n",
    "  - Flags that control debugging and verbosity of logging.  \n",
    "\n",
    "  - **`DEFAULT_VERBOSE`**: Displays general information during execution.  \n",
    "\n",
    "  - **`DEFAULT_DEBUG`**: Provides more detailed diagnostic information.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **`build_dummy_pydantic_object`**   \n",
    "\n",
    "#### **Purpose**  \n",
    "\n",
    "- Generates a placeholder object using a given **Pydantic** schema.  \n",
    "\n",
    "- Useful for creating default or mock data based on a schema.  \n",
    "\n",
    "\n",
    "#### **Parameters**  \n",
    "\n",
    "- **`schema`**: A **Pydantic** schema class that defines the structure of the object.  \n",
    "\n",
    "#### **Return Value**  \n",
    "\n",
    "- An instance of the **Pydantic** schema populated with default values.  \n",
    "\n",
    "#### **Implementation**  \n",
    "\n",
    "- Calls the schema class to create an object instance using its default attributes.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **`generate_object`**   \n",
    "\n",
    "#### **Purpose**  \n",
    "\n",
    "- Communicates with the AI model using the provided prompt and a structured response schema.  \n",
    "\n",
    "#### **Parameters**  \n",
    "\n",
    "- **`prompt`**: The input query for the AI model.  \n",
    "\n",
    "- **`response_model`**: A **Pydantic** schema defining the structure of the expected output.\n",
    "\n",
    "- **`system`, `model`, `temperature`, `max_tokens`**: Optional arguments to customize the AI's behavior.  \n",
    "\n",
    "- **`debug`, `verbose`**: Flags to control logging and diagnostic output.  \n",
    "\n",
    "\n",
    "#### **Return Value**  \n",
    "\n",
    "- Returns a validated **Pydantic** object representing the model's response, or **`None`** if an error occurs.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Generating and Debugging AI Response  \n",
    "\n",
    "#### **Purpose**  \n",
    "\n",
    "- Constructs a detailed prompt for the AI, including the input query and the expected schema.  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`build_dummy_pydantic_object(response_model).model_dump_json():`**  \n",
    "\n",
    "  - Converts a dummy object of the response schema into a JSON string to clarify the desired format for the AI.  \n",
    "\n",
    "---\n",
    "\n",
    "### 6. Debugging Parameters  \n",
    "\n",
    "#### **Purpose**  \n",
    "\n",
    "- Logs the parameters being sent to the AI for debugging purposes.  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`json.dumps(params, indent=2):`**  \n",
    "\n",
    "  - Formats the parameters dictionary as a readable JSON string with 2-space indentation.  \n",
    "\n",
    "---\n",
    "\n",
    "### 7. Communicating with the AI Model  \n",
    "\n",
    "#### **Purpose**  \n",
    "\n",
    "- Sends the query and schema to the **`ollama.chat`** function to interact with the specified AI model.  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`messages:`**  \n",
    "\n",
    "  - A list of messages simulating a conversation.  \n",
    "\n",
    "  - The role of **`\"user\"`** indicates the input is a user query.  \n",
    "\n",
    "- **`format=\"json\":`**  \n",
    "\n",
    "  - Specifies that the response should be formatted as **JSON**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 8. Processing the Response  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`response.message.content:`**  \n",
    "\n",
    "  - Retrieves the raw content of the model's response.  \n",
    "\n",
    "- **`json.loads(response_json):`**  \n",
    "\n",
    "  - Parses the **JSON** string into a Python dictionary.  \n",
    "\n",
    "- **`response_model.model_validate(response_obj):`**  \n",
    "\n",
    "  - Validates and converts the parsed **JSON** response into a structured **Pydantic** object.  \n",
    "\n",
    "---\n",
    "\n",
    "### 9. Exception Handling  \n",
    "\n",
    "#### **Purpose**  \n",
    "\n",
    "- Catches errors that may occur during execution, such as invalid **JSON** or schema mismatches.  \n",
    "\n",
    "#### **Code**  \n",
    "\n",
    "- **`traceback.print_exc():`**  \n",
    "\n",
    "  - Prints a detailed stack trace to help diagnose the issue.  \n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion  \n",
    "\n",
    "- This code integrates structured data validation (**Pydantic**) with an AI model interface (**ollama.chat**).  \n",
    "\n",
    "- It provides a robust way to interact with the model while ensuring the output adheres to a predefined schema, enhancing reliability and usability in applications.  \n",
    "\n",
    "- Debugging and error-handling mechanisms ensure smooth operation even when issues arise.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an intelligent assistant. You are helping the user with their query.\"\n",
    ")\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 100\n",
    "DEFAULT_OLLAMA_MODEL = \"phi4\"\n",
    "DEFAULT_VERBOSE = True\n",
    "DEFAULT_DEBUG = True\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "def generate_object(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=DEFAULT_SYSTEM_PROMPT,\n",
    "    model=DEFAULT_OLLAMA_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    debug=DEFAULT_DEBUG,\n",
    "    verbose=DEFAULT_VERBOSE,\n",
    ") -> Union[BaseModel, None]:\n",
    "    \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "    try:\n",
    "        if verbose or debug:\n",
    "            print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "        prompt_with_structured_output = f\"\"\"\n",
    "            Prompt: {prompt} \n",
    "            SCHEMA: {build_dummy_pydantic_object(response_model).model_dump_json()}\n",
    "            RESPOND IN JSON FORMAT\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            params = {\n",
    "                \"prompt\": prompt_with_structured_output,\n",
    "                \"system\": system,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "            }\n",
    "            params = json.dumps(params, indent=2)\n",
    "            print(f\"Params: {params}\")\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "        response_json = response.message.content  # Get the response content\n",
    "        print(response_json)\n",
    "        response_obj = json.loads(response_json)\n",
    "        response_structured = response_model.model_validate(response_obj)\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(\"Object generated successfully. üéâ\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"EasyLLM Response: {response_json}\")\n",
    "        return response_structured\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Validating JSON Using Pydantic Models\n",
    "\n",
    "This block uses Pydantic to define a schema for a joke object, including a joke and its author. The assistant is prompted to generate a joke that adheres to this schema. The structured JSON response is validated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = (\n",
    "        \"Why did the scarecrow win an award? Because he was outstanding in his field.\"\n",
    "    )\n",
    "    author: str = \"Unknown\"\n",
    "\n",
    "\n",
    "response = generate_object(\"Tell me a joke\", response_model=Joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Requesting and Validating AI-Generated Joke with Pydantic Schema  \n",
    "\n",
    "---  \n",
    "\n",
    "### Importing Necessary Libraries  \n",
    "\n",
    "- **`import ollama:`**  \n",
    "\n",
    "  - Imports the **`ollama`** library, which is used to interact with an AI model for natural language processing tasks.  \n",
    "\n",
    "- **`from pydantic import BaseModel:`**  \n",
    "\n",
    "  - Imports the **`BaseModel`** class from the **`Pydantic`** library.  \n",
    "\n",
    "  - **`BaseModel`** helps in defining data schemas and validating data.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Defining the Joke Model  \n",
    "\n",
    "- **`class Joke(BaseModel):`**  \n",
    "\n",
    "  - Defines a **Pydantic** model class called **`Joke`** that inherits from **`BaseModel`**.  \n",
    "\n",
    "  - This model will represent the structure of the joke response from the AI.  \n",
    "\n",
    "- **`joke: str`** and **`author: str:`**  \n",
    "\n",
    "  - These are the attributes of the **`Joke`** class.  \n",
    "\n",
    "  - **`joke`** is a string containing the joke text.  \n",
    "\n",
    "  - **`author`** is a string containing the name of the author of the joke.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Constructing the Prompt  \n",
    "\n",
    "- **`prompt = f\"\"\":`**  \n",
    "\n",
    "  - Defines a multi-line formatted string (**f-string**) that will be used to interact with the AI model.  \n",
    "\n",
    "- **`\"Tell me a joke.\":`**  \n",
    "\n",
    "  - The user query instructs the AI to tell a joke.  \n",
    "\n",
    "- **`Respond with the schema: {Joke.model_json_schema()}:`**  \n",
    "\n",
    "  - The AI is instructed to respond in a specific format, which is defined by the **`Joke`** model.  \n",
    "\n",
    "  - **`Joke.model_json_schema()`** dynamically inserts the **JSON schema** of the **Joke** model (i.e., the structure of the joke response) into the prompt.  \n",
    "\n",
    "- **`Include only the keys and values and not the schema itself.:`**  \n",
    "\n",
    "  - Instructs the AI to return only the values (**joke** and **author**), excluding the schema structure.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Generating the Response Using the generate_object Function  \n",
    "\n",
    "- **`response = generate_object(prompt, response_model=Joke):`**  \n",
    "\n",
    "  - Calls the **`generate_object`** function (defined in the previous code) to send the prompt to the AI model.  \n",
    "\n",
    "  - The **`response_model=Joke`** argument ensures the response is validated against the **Joke** model.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Printing the Response  \n",
    "\n",
    "- **`print(response):`**  \n",
    "\n",
    "  - Prints the structured response returned by the **`generate_object`** function.  \n",
    "\n",
    "  - The response should be a **Joke** object containing the joke and the author.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Conclusion  \n",
    "\n",
    "- This code constructs a structured prompt to request a joke from the AI and formats the response according to a predefined **Pydantic** schema (**Joke**).  \n",
    "\n",
    "- By using the **`generate_object`** function, the response is validated and ensures that only the required data (the joke and author) is returned in a clean and structured format.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "    author: str\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema: {Joke.model_json_schema()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "\"\"\"\n",
    "\n",
    "response = generate_object(prompt, response_model=Joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generating and Validating AI-Generated Joke with Schema Enforcement\n",
    "\n",
    "---  \n",
    "\n",
    "### Importing Necessary Libraries  \n",
    "\n",
    "- **`import ollama:`**  \n",
    "\n",
    "  - Imports the **`ollama`** library, which is used for interacting with AI models like **phi4** for natural language processing tasks.  \n",
    "\n",
    "- **`from pydantic import BaseModel:`**  \n",
    "\n",
    "  - Imports the **`BaseModel`** class from the **Pydantic** library.  \n",
    "\n",
    "  - **`BaseModel`** helps in defining schemas and validating data for structured responses.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Defining the Joke Model  \n",
    "\n",
    "- **`class Joke(BaseModel):`**  \n",
    "\n",
    "  - Defines a **Pydantic** model class called **Joke**, inheriting from **BaseModel**.  \n",
    "\n",
    "  - The **Joke** model will structure the response containing a joke and its author.  \n",
    "\n",
    "- **`joke: str = \"A joke\"`** and **`author: str = \"An author\":`**  \n",
    "\n",
    "  - These are default attributes of the **Joke** class.  \n",
    "\n",
    "  - **`joke`** holds a string that represents the joke text, with a default value **\"A joke\"**.  \n",
    "\n",
    "  - **`author`** holds a string for the author of the joke, with a default value **\"An author\"**.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Building a Dummy Pydantic Object  \n",
    "\n",
    "- **`def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:`**  \n",
    "\n",
    "  - Defines a helper function **`build_dummy_pydantic_object`** that creates an instance of a **Pydantic** model (in this case, **Joke**) with dummy values.  \n",
    "\n",
    "- **Function Explanation:**  \n",
    "\n",
    "  - The function takes a **schema** (which is a **Pydantic** model) as an argument.  \n",
    "\n",
    "  - It returns an instance of the provided schema initialized with its default values.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Constructing the Prompt  \n",
    "\n",
    "- **`prompt = f\"\"\":`**  \n",
    "\n",
    "  - Defines a formatted string (**f-string**) to create a structured prompt for the AI.  \n",
    "\n",
    "- **`\"Tell me a joke.\":`**  \n",
    "\n",
    "  - The main instruction for the AI to generate a joke.  \n",
    "\n",
    "- **`Respond with the schema ‚Äî Example object: {build_dummy_pydantic_object(Joke).model_dump_json()}:`**  \n",
    "\n",
    "  - This part asks the AI to respond in the format defined by the **Joke** schema.  \n",
    "\n",
    "  - **`build_dummy_pydantic_object(Joke).model_dump_json()`** provides an example object in **JSON** format, which is used to show the AI how to structure the response.  \n",
    "\n",
    "- **`Include only the keys and values and not the schema itself.:`**  \n",
    "\n",
    "  - Instructs the AI to return just the values for the **joke** and **author** fields, without the schema definition.  \n",
    "\n",
    "- **`Respond strictly in the given schema format.:`**  \n",
    "\n",
    "  - Ensures that the AI‚Äôs response adheres strictly to the structure of the **Joke** schema.  \n",
    "\n",
    "- **`Respond only with valid JSON and don't include anything else.:`**  \n",
    "\n",
    "  - Further enforces the requirement for the AI to return a clean, well-formed **JSON** response, without any extraneous information.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Generating the Response Using the ollama.chat Function  \n",
    "\n",
    "- **`response = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}]):`**  \n",
    "\n",
    "  - Sends the structured prompt to the **phi4** AI model using **ollama.chat**.  \n",
    "\n",
    "  - The AI responds with a message formatted according to the instructions provided in the prompt.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Printing the Response  \n",
    "\n",
    "- **`print(response.message.content):`**  \n",
    "\n",
    "  - Prints the content of the response message returned by the AI.  \n",
    "\n",
    "  - This will be the joke along with the author's name in **JSON** format, strictly following the schema.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Conclusion  \n",
    "\n",
    "- This code interacts with the **phi4** AI model to generate a joke and its author, ensuring that the response strictly adheres to the predefined **Joke** schema.  \n",
    "\n",
    "- The response is validated against the schema and returned in a clean **JSON** format, which only includes the joke and author, excluding the schema structure itself.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = \"A joke\"\n",
    "    author: str = \"An author\"\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema ‚Äî Example object: {build_dummy_pydantic_object(Joke).model_dump_json()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "Respond strictly in the given schema format.\n",
    "Respond only with valid JSON and don't include anything else.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook effectively demonstrates how to integrate structured data processing with language models. The approach ensures schema validation, making it suitable for applications like API testing, structured data generation, and schema-driven conversations. The use of Pydantic ensures type safety and consistency, while Ollama's models bring the power of conversational AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
