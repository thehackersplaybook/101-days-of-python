{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesla Smart Routing Reporter\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "\n",
    "The Tesla Smart Routing Reporter is a Python-based intelligent system designed to analyze travel routes, traffic, and energy consumption for Tesla vehicles using a JSON dataset. It leverages OpenAI's GPT API integrated with the Toolhouse system design tools to process data, provide insights, and generate visually appealing reports. The app automates the evaluation of routes, calculates energy efficiency, and provides actionable information in markdown reports for electric vehicle optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Boilerplate And Requirements Setup:\n",
    "\n",
    "This code sets up the environment for a Jupyter notebook:\n",
    "\n",
    "- `Install Requirements`:\n",
    "The install_requirements() function checks if the required dependencies from requirements.txt are installed. If not, it installs them and retries up to three times if the installation fails.\n",
    "\n",
    "- `Environment Setup`:\n",
    "The setup_env() function loads environment variables from a .env file using load_dotenv(). It checks if essential environment variables like OPENAI_API_KEY and TOOLHOUSE_API_KEY are set, exiting if any are missing.\n",
    "\n",
    "- `Final Setup`:\n",
    "Once the requirements are installed and environment variables validated, the setup is marked complete with a success message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\", \"TOOLHOUSE_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"üöÄ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: API Integration and Toolhouse Setup\n",
    "\n",
    "This step integrates OpenAI's GPT model with the Toolhouse bundle, using tools like Sendgrid, Web Scraper, and Code Interpreter for enhanced functionality. \n",
    "\n",
    "The `llm()` function interacts with these tools, iterating until a final response is produced or the maximum iterations are reached. \n",
    "\n",
    "It includes error handling and allows for verbose output or intermediate responses for debugging and transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: LLM with Toolhouse\n",
    "## Include the following tools in the Toolhouse bundle: thp-system-design-assistant\n",
    "## Sendgrid, Web Scraper, Code Interpreter, and Memory tools are included in this bundle.\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from toolhouse import Toolhouse\n",
    "import traceback\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TOOLHOUSE_API_KEY = os.getenv(\"TOOLHOUSE_API_KEY\")\n",
    "MAX_ITERS = 10\n",
    "th = Toolhouse(api_key=TOOLHOUSE_API_KEY, provider=\"openai\")\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "def llm(\n",
    "    prompt: str,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system=\"You are a helpful AI assistant\",\n",
    "    intermediate_output=True,\n",
    "    verbose=True,\n",
    ") -> str:\n",
    "    \"\"\"Wrapper over Open AI LLM calls with Toolhouse Tools.\"\"\"\n",
    "    try:\n",
    "        if verbose:\n",
    "            print(\"System: \", system)\n",
    "            print(\"Prompt: \", prompt)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=th.get_tools(\"thp-system-design-assistant\"),\n",
    "        )\n",
    "        messages += th.run_tools(response)\n",
    "\n",
    "        tool_calls = response.choices[0].message.tool_calls\n",
    "        content = response.choices[0].message.content\n",
    "        iters = 0\n",
    "\n",
    "        while tool_calls and len(tool_calls) > 0 and iters < MAX_ITERS:\n",
    "            print(\"Tools: \", tool_calls)\n",
    "            if verbose or intermediate_output:\n",
    "                if content:\n",
    "                    print(f\"Cycle {iters}: \", content)\n",
    "                else:\n",
    "                    print(f\"Cycle {iters}: No content.\")\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=th.get_tools(\"thp-system-design-assistant\"),\n",
    "            )\n",
    "            messages += th.run_tools(response)\n",
    "            if verbose or intermediate_output:\n",
    "                print(f\"Generated intermediate response...\")\n",
    "            content = response.choices[0].message.content\n",
    "            tool_calls = response.choices[0].message.tool_calls\n",
    "            iters += 1\n",
    "\n",
    "        if content and (verbose or intermediate_output):\n",
    "            print(f\"Final response: {content}\")\n",
    "\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return \"Sorry, failed to generate response.\\nError: \" + str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating Routing Data\n",
    "\n",
    "### Key Elements\n",
    "\n",
    "- **ROUTING_DATA**:\n",
    "\n",
    "  - **locations**: List of locations with names and coordinates.\n",
    "\n",
    "  - **traffic_data**: Distance, traffic level, and speed data between locations.\n",
    "\n",
    "  - **elevation_data**: Elevation details for each location.\n",
    "\n",
    "  - **vehicle_specs**: Tesla Model 3's battery capacity and efficiency.\n",
    "\n",
    "- **prompts**:\n",
    "\n",
    "  - Prompts for loading, displaying, visualizing data, calculating travel times, energy usage, and comparing routes based on efficiency, battery consumption, and other factors.\n",
    "\n",
    "### Potential Use Cases\n",
    "\n",
    "- **Route Analysis**: \n",
    "\n",
    "  - Compare routes based on travel time, energy consumption, and traffic.\n",
    "\n",
    "- **Energy Efficiency**: \n",
    "\n",
    "  - Find the most energy-efficient route.\n",
    "\n",
    "- **Battery Management**: \n",
    "\n",
    "  - Check how battery consumption varies with traffic and elevation, and if locations are reachable with available battery.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- This setup supports analyzing and visualizing EV travel and energy consumption for route planning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tesla Smart Routing Reporter\n",
    "\n",
    "ROUTING_DATA = {\n",
    "    \"locations\": [\n",
    "        {\"id\": \"A\", \"coordinates\": [37.7749, -122.4194], \"name\": \"San Francisco\"},\n",
    "        {\"id\": \"B\", \"coordinates\": [34.0522, -118.2437], \"name\": \"Los Angeles\"},\n",
    "        {\"id\": \"C\", \"coordinates\": [36.7783, -119.4179], \"name\": \"Fresno\"},\n",
    "    ],\n",
    "    \"traffic_data\": [\n",
    "        {\n",
    "            \"from\": \"A\",\n",
    "            \"to\": \"C\",\n",
    "            \"distance_km\": 300,\n",
    "            \"traffic_level\": \"moderate\",\n",
    "            \"speed_kmph\": 80,\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"C\",\n",
    "            \"to\": \"B\",\n",
    "            \"distance_km\": 200,\n",
    "            \"traffic_level\": \"heavy\",\n",
    "            \"speed_kmph\": 50,\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"A\",\n",
    "            \"to\": \"B\",\n",
    "            \"distance_km\": 500,\n",
    "            \"traffic_level\": \"light\",\n",
    "            \"speed_kmph\": 100,\n",
    "        },\n",
    "    ],\n",
    "    \"elevation_data\": [\n",
    "        {\"id\": \"A\", \"elevation_m\": 16},\n",
    "        {\"id\": \"B\", \"elevation_m\": 71},\n",
    "        {\"id\": \"C\", \"elevation_m\": 94},\n",
    "    ],\n",
    "    \"vehicle_specs\": {\n",
    "        \"model\": \"Tesla Model 3\",\n",
    "        \"efficiency_wh_per_km\": 150,\n",
    "        \"battery_capacity_wh\": 75000,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Load this JSON dataset and display the locations, traffic, and elevation data in a readable format.\",\n",
    "    \"Create a map visualizing the routes between the locations A, B, and C with distances and traffic levels as labels.\",\n",
    "    \"Calculate the travel time for each route considering traffic and average speeds provided in the dataset. Provide the result in a readable format.\",\n",
    "    \"Write a formula to calculate additional energy usage for elevation gains, assuming that for every 100 meters of elevation gain, the vehicle uses an additional 10 Wh per km.\",\n",
    "    \"Compute the most energy-efficient route from San Francisco (A) to Los Angeles (B) using distance, traffic, speed, and elevation data.\",\n",
    "    \"Provide a table comparing all possible routes between A and B with metrics like travel time, elevation gain, energy consumption (Wh), and remaining battery percentage.\",\n",
    "    \"Simulate how changes in traffic levels or elevation impacts the energy consumption for the A to B route.\",\n",
    "    \"Plot a bar chart showing energy consumption for each route between A and B.\",\n",
    "    \"Create a heatmap showing locations reachable from A with 80% battery given the vehicle's specs.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: File Existence and Section Check:\n",
    "\n",
    "In this step, two key functions are defined:\n",
    "\n",
    "- `is_section_in_file(section_hint, file_path)`: Checks if a specific section exists in a file by searching for a section_hint within the file content.\n",
    "\n",
    "- `create_file_if_not_exists_recursive(file_path)`: Creates a file along with any necessary parent directories if the file doesn't already exist.\n",
    "\n",
    "Both functions are equipped with error handling to capture and print any issues, ensuring smooth execution and providing valuable debugging information when problems arise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "output_file = \"test_output/tesla_routing_output.md\"  # change as required\n",
    "\n",
    "\n",
    "def is_section_in_file(section_hint: str, file_path: str) -> bool:\n",
    "    \"\"\"Check if a section hint is present in a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        return section_hint in content\n",
    "    except Exception as e:\n",
    "        print(\"Error reading file:\", str(e))\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_file_if_not_exists_recursive(file_path: str):\n",
    "    \"\"\"Create a file if it does not exist, creating parent directories if necessary.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(\"\")\n",
    "    except Exception as e:\n",
    "        print(\"Error creating file:\", str(e))\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prompt Handling and Response Logging:\n",
    "\n",
    "The code ensures the output file exists and checks if each prompt has already been processed. If not, it sends the prompt with ROUTING_DATA to the LLM, then appends the response to the file in Markdown format. \n",
    "\n",
    "Once all prompts are processed, it notifies the user that the task is complete and the output file is ready for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_if_not_exists_recursive(output_file)\n",
    "\n",
    "for prompt in prompts:\n",
    "    if is_section_in_file(prompt, output_file):\n",
    "        print(f\"Skipping prompt: {prompt}\")\n",
    "        continue\n",
    "    data_json = json.dumps(ROUTING_DATA)\n",
    "    prompt = f\"{prompt}. Provide the response in a readable format.\\n\\n```json\\n{data_json}\\n```\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    response = llm(prompt=prompt)\n",
    "    print(\"Response: \", response)\n",
    "    if not is_section_in_file(prompt, output_file):\n",
    "        with open(output_file, \"a\") as f:\n",
    "            f.write(f\"## {prompt}\\n\" + response + \"\\n\\n\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    f\"üöÄ All prompts completed. Check the output file ('{output_file}') for the responses.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The Tesla Smart Routing Reporter is a powerful tool for optimizing Tesla Model 3 routes by analyzing traffic, elevation, and energy consumption data. It efficiently processes JSON datasets, calculates travel times, energy efficiency, and provides actionable insights. By leveraging OpenAI's GPT models and Toolhouse, the app automates the generation of detailed reports, visualizations, and comparisons, ultimately helping drivers make smarter decisions for energy-efficient routes and better vehicle performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
