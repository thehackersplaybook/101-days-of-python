{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Base Builder\n",
        "\n",
        "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
        "<p style=\"margin-right:10px;\">\n",
        "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
        "</p>\n",
        "</div>\n",
        "\n",
        "## Description:\n",
        "\n",
        "The Knowledge Base Builder is a powerful tool designed to scrape and organize valuable information from various online resources, including blogs and knowledge bases.\n",
        "\n",
        "### Features:\n",
        "\n",
        "- **Web Scraping**:  \n",
        "  Utilizes BeautifulSoup for scraping data from web pages, ensuring that relevant content is extracted.\n",
        "\n",
        "- **Markdown Conversion**:  \n",
        "  Converts scraped content into markdown format, making it easily readable and portable.\n",
        "\n",
        "- **YouTube Integration**:  \n",
        "  Integrates YouTube video transcripts to provide a comprehensive knowledge collection.\n",
        "\n",
        "- **Environment Setup**:  \n",
        "  Ensures all necessary environment variables are set and automatically installs required dependencies for smooth operation.\n",
        "\n",
        "- **Data Storage**:  \n",
        "  Once the data is fetched, it is saved in a markdown file, creating a structured knowledge base.\n",
        "\n",
        "### Use Cases:\n",
        "- **Knowledge Aggregation**:  \n",
        "  Collect information from various sources into one centralized repository.\n",
        "\n",
        "- **Personal/Team Knowledge Repositories**:  \n",
        "  Build a knowledge base for personal or team use, making information accessible and organized.\n",
        "\n",
        "- **Customizable Domains**:  \n",
        "  The app can be customized to gather data from specific knowledge domains, ensuring it meets your needs.\n",
        "\n",
        "### Summary:\n",
        "This application provides a streamlined process for building and organizing knowledge bases, automating data collection, formatting, and storage into markdown files for easy access and management.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Installing dependencies\n",
        "\n",
        "This block of code handles the setup process for the notebook. It first installs the required Python packages listed in the `requirements.txt` file using `pip`. If the installation is successful, it moves on to loading environment variables from a `.env` file. It then checks if the necessary environment variables (such as API keys) are set, and if any are missing, it prompts the user to set them. Finally, it ensures that the environment is ready for the rest of the notebook to execute, clearing output to maintain a clean start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boilerplate: This block goes into every notebook.\n",
        "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "requirements_installed = False\n",
        "max_retries = 3\n",
        "retries = 0\n",
        "REQUIRED_ENV_VARS = []\n",
        "\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
        "    global requirements_installed\n",
        "    if requirements_installed:\n",
        "        print(\"Requirements already installed.\")\n",
        "        return\n",
        "\n",
        "    print(\"Installing requirements...\")\n",
        "    install_status = os.system(\"pip install -r requirements.txt\")\n",
        "    if install_status == 0:\n",
        "        print(\"Requirements installed successfully.\")\n",
        "        requirements_installed = True\n",
        "    else:\n",
        "        print(\"Failed to install requirements.\")\n",
        "        if retries < max_retries:\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            return install_requirements()\n",
        "        exit(1)\n",
        "    return\n",
        "\n",
        "\n",
        "def setup_env():\n",
        "    \"\"\"Sets up the environment variables\"\"\"\n",
        "\n",
        "    def check_env(env_var):\n",
        "        value = os.getenv(env_var)\n",
        "        if value is None:\n",
        "            print(f\"Please set the {env_var} environment variable.\")\n",
        "            exit(1)\n",
        "        else:\n",
        "            print(f\"{env_var} is set.\")\n",
        "\n",
        "    load_dotenv(override=True)\n",
        "\n",
        "    variables_to_check = REQUIRED_ENV_VARS\n",
        "\n",
        "    for var in variables_to_check:\n",
        "        check_env(var)\n",
        "\n",
        "\n",
        "install_requirements()\n",
        "clear_output()\n",
        "setup_env()\n",
        "print(\"üöÄ Setup complete. Continue to the next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Web Scraping and Content Extraction\n",
        "\n",
        "This block of code defines functions to scrape and extract web content efficiently. It fetches links from a specified URL, caches them for reuse, and processes them in batches to avoid redundant web requests. For individual URLs, the code checks if the content is already cached, and if not, it retrieves the page content, converts it to Markdown, and stores it in a cache. Additionally, it handles YouTube video URLs by fetching and formatting video transcripts into readable text. These steps ensure optimized data extraction and reduce unnecessary web traffic by leveraging caching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from typing import List\n",
        "import traceback\n",
        "from markdownify import markdownify as md\n",
        "from youtube_transcript_api.formatters import TextFormatter\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "formatter = TextFormatter()\n",
        "\n",
        "cache = {}\n",
        "\n",
        "\n",
        "def get_base_url(url):\n",
        "    return \"/\".join(url.split(\"/\")[:3])\n",
        "\n",
        "\n",
        "def get_links_from_page(url):\n",
        "    global cache\n",
        "    try:\n",
        "        cached_item = cache.get(url)\n",
        "        cached_urls = cached_item.get(\"urls\") if cached_item else None\n",
        "        if cached_urls:\n",
        "            print(f\"Returning cached links for {url}\")\n",
        "            return cached_urls, None\n",
        "        base_url = get_base_url(url)\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        links = []\n",
        "        for link in soup.find_all(\"a\"):\n",
        "            cur_link = link.get(\"href\")\n",
        "            if not cur_link:\n",
        "                continue\n",
        "            if not cur_link.startswith(\"http\"):\n",
        "                links.append(f\"{base_url}{cur_link}\")\n",
        "            else:\n",
        "                links.append(cur_link)\n",
        "        if not cached_item:\n",
        "            cache[url] = {\n",
        "                \"urls\": links,\n",
        "                \"content\": None,\n",
        "                \"url\": url,\n",
        "            }\n",
        "        else:\n",
        "            cached_item[\"urls\"] = links\n",
        "            cache[url] = cached_item\n",
        "        return links, None\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get links from {url}. Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return [], str(e)\n",
        "\n",
        "\n",
        "def get_page_content(url):\n",
        "    try:\n",
        "        cached_item = cache.get(url)\n",
        "        cached_content = cached_item.get(\"content\") if cached_item else None\n",
        "        if cached_content:\n",
        "            print(f\"Returning cached content for {url}\")\n",
        "            return cached_content, None\n",
        "        if \"youtube\" in url or \"youtu.be\" in url or \"youtube.com\" in url:\n",
        "            video_id = url.split(\"=\")[-1]\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "            text_transcript = formatter.format_transcript(transcript)\n",
        "            return md(text_transcript), None\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        result = md(str(soup))\n",
        "        if not cached_item:\n",
        "            cache[url] = {\n",
        "                \"urls\": None,\n",
        "                \"content\": result,\n",
        "                \"url\": url,\n",
        "            }\n",
        "        else:\n",
        "            cached_item[\"content\"] = result\n",
        "            cache[url] = cached_item\n",
        "        return result, None\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get content from {url}. Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return \"\", str(e)\n",
        "\n",
        "\n",
        "def get_page_content_batch(urls: List[str]):\n",
        "    results = []\n",
        "    for url in urls:\n",
        "        cached_item = cache.get(url)\n",
        "        cached_content = cached_item.get(\"content\") if cached_item else None\n",
        "        if cached_content:\n",
        "            print(f\"Returning cached content for {url}\")\n",
        "            results.append({\"url\": url, \"content\": cached_content, \"error\": None})\n",
        "            continue\n",
        "        print(f\"Getting content from {url}\")\n",
        "        content, error = get_page_content(url)\n",
        "        results.append({\"url\": url, \"content\": content, \"error\": error})\n",
        "        if not cached_item:\n",
        "            cache[url] = {\n",
        "                \"urls\": None,\n",
        "                \"content\": content,\n",
        "                \"url\": url,\n",
        "            }\n",
        "        else:\n",
        "            cached_item[\"content\"] = content\n",
        "            cache[url] = cached_item\n",
        "        result_bytes_count = len(content.encode(\"utf-8\"))\n",
        "        print(f\"Content from {url} fetched. Size: {result_bytes_count} bytes!\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Knowledge Base Extraction and Compilation\n",
        "\n",
        "This step fetches blog links and content from the AE (Arpit Bhayani) website, cleaning and organizing the content. It builds a knowledge base by fetching relevant pages, extracting their text content, and saving it into a markdown file. The process ensures that all data is structured and saved for further use or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "knowledge_base_cache = {}\n",
        "\n",
        "\n",
        "def get_ae_blog_links():\n",
        "    blogs_base_url = \"https://arpitbhayani.me/blogs\"\n",
        "    blog_links = []\n",
        "    links, error = get_links_from_page(blogs_base_url)\n",
        "    if error:\n",
        "        return blog_links, error\n",
        "    blog_links.extend(links)\n",
        "    return blog_links, None\n",
        "\n",
        "\n",
        "def clean_whitespace(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans the whitespace as per following rules;\n",
        "    - Removes leading and trailing whitespaces.\n",
        "    - Replaces multiple whitespaces with a single whitespace.\n",
        "    - Replaces multiple newlines with a single newline.\n",
        "    - Removes any leading or trailing newlines.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    multiple_whitespaces_remover = \" \".join(text.split())\n",
        "    return (\n",
        "        multiple_whitespaces_remover.replace(\"\\n \", \"\\n\").replace(\" \\n\", \"\\n\").strip()\n",
        "    )\n",
        "\n",
        "\n",
        "def fetch_knowledge_base():\n",
        "    knowledge_base_link = \"https://arpitbhayani.me/knowledge-base\"\n",
        "    links_to_fetch = []\n",
        "    sub_page_links, error = get_links_from_page(knowledge_base_link)\n",
        "    if error:\n",
        "        return [], error\n",
        "    for link in sub_page_links:\n",
        "        is_knowledge_base_link = \"knowledge-base\" in link\n",
        "        if not is_knowledge_base_link:\n",
        "            continue\n",
        "        is_google_drive_link = \"drive.google.com\" in link\n",
        "        if is_google_drive_link:\n",
        "            continue\n",
        "        result_links = []\n",
        "        try:\n",
        "            blog_links, error = get_links_from_page(link)\n",
        "            if error:\n",
        "                print(f\"Failed to get links from {link}. Error: {error}\")\n",
        "                continue\n",
        "            result_links.extend(blog_links)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to get links from {link}. Error: {e}\")\n",
        "            traceback.print_exc()\n",
        "        links_to_fetch.extend(result_links)\n",
        "    result = get_page_content_batch(links_to_fetch)\n",
        "    return result\n",
        "\n",
        "\n",
        "def build_knowledge_base(output_file=\"ae_knowledge_base.md\"):\n",
        "    try:\n",
        "        now = datetime.now()\n",
        "        now_human_formtted = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "        output_file_name, extension = os.path.splitext(output_file)\n",
        "        output_file = f\"{output_file_name}_{now_human_formtted}{extension}\"\n",
        "        file_exists = os.path.exists(output_file)\n",
        "        if not file_exists:\n",
        "            with open(output_file, \"w\") as f:\n",
        "                f.write(\"# AE Knowledge Base\\n\\n\")\n",
        "        blog_links, error = get_ae_blog_links()\n",
        "        if error:\n",
        "            return [], error\n",
        "        all_content = []\n",
        "        content = get_page_content_batch(blog_links)\n",
        "        knowledge_base_content = fetch_knowledge_base()\n",
        "        all_content.extend(knowledge_base_content)\n",
        "        with open(output_file, \"w\") as f:\n",
        "            for c in all_content:\n",
        "                # c['content'] = clean_whitespace(c['content'])\n",
        "                f.write(f\"# {c['url']}\\n\")\n",
        "                f.write(c[\"content\"])\n",
        "                f.write(\"\\n\\n\")\n",
        "        return content, None\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to build knowledge base. Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return [], str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Fetch AE Blog Links\n",
        "\n",
        "This step retrieves the list of blog links from the AE website by calling the `get_ae_blog_links()` function. It then prints the total number of links found, providing a preview of the available blog resources. The result is a collection of blog links that will be processed further for content extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ae_links, error = get_ae_blog_links()\n",
        "\n",
        "print(f\"Found {len(ae_links)} links\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fetch Knowledge Base Content\n",
        "\n",
        "In this step, the function `fetch_knowledge_base()` is called to fetch content from various links within the knowledge base. The retrieved content is stored in `knowledge_base_content`, and the number of links fetched is printed to give an overview of the amount of knowledge base material collected. This content is later used to build the comprehensive knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "knowledge_base_content = fetch_knowledge_base()\n",
        "\n",
        "print(f\"Fetched {len(knowledge_base_content)} links.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Build Knowledge Base\n",
        "\n",
        "In this step, the `build_knowledge_base()` function is called to create the knowledge base by collecting and processing content from various sources. The output is saved in a markdown file named `ae_knowledge_base_v0_0_2.md`. If the process is successful, it prints the success message along with the number of sections in the content; otherwise, it handles errors and displays the failure message. This step finalizes the creation of the knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = \"ae_knowledge_base_v0_0_2.md\"\n",
        "contents, error = build_knowledge_base(output_file=output_file)\n",
        "\n",
        "\n",
        "if error:\n",
        "    print(f\"Failed to build knowledge base. Error: {error}\")\n",
        "else:\n",
        "    print(f\"Knowledge base built successfully. Output file: {output_file}\")\n",
        "    print(f\"Content: {len(contents)} sections. \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To-Do Tasks for Knowledge Base Integration:\n",
        "\n",
        "This step outlines the remaining tasks to complete the project :\n",
        "\n",
        "\n",
        "- Load and clean the knowledge base content: This involves processing the fetched knowledge base data to ensure it is well-structured and free of any unwanted formatting or inconsistencies.\n",
        "\n",
        "- Load the knowledge base content into a vector store: This step stores the cleaned content in a vector format, allowing efficient searching and retrieval.\n",
        "\n",
        "- Implement a simple RAG (Retrieval-Augmented Generation) using Phi4 and the vector store: This introduces the use of RAG for enhancing response quality by integrating the vector store with a retrieval mechanism.\n",
        "\n",
        "- Implement optimizations on the RAG to improve response quality: This final task focuses on fine-tuning the RAG system for better performance and more accurate results in real-world applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# TODO: Things to do till this is complete.\n",
        "# - Load and clean the knowledge base content.\n",
        "# - Load the knowledge base content into a vector store.\n",
        "# - Implement a simple RAG using Phi4 and the vector store.\n",
        "# - Implement any optimizations on the RAG to improve response quality.\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion:\n",
        "\n",
        "This app serves as a comprehensive tool for building a knowledge base from various online sources, specifically blogs and knowledge base pages. By leveraging the `BeautifulSoup` and `YouTubeTranscriptApi` libraries, it efficiently extracts, processes, and cleans content, storing it in a markdown format. The app also includes mechanisms for caching to optimize repeated fetches. Moving forward, the next steps involve implementing a vector store to load the knowledge base and utilizing Retrieval-Augmented Generation (RAG) for improved response quality, paving the way for advanced AI-driven applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Thank You for visiting The Hackers Playbook! üåê\n",
        "\n",
        "If you liked this research material;\n",
        "\n",
        "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
        "\n",
        "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
        "\n",
        "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
        "\n",
        "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
        "<p style=\"margin-right:10px;\">\n",
        "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
        "</p>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
