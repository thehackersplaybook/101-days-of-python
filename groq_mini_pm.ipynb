{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniPM: AI-Powered Project Management Assistant\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "\n",
    "MiniPM is an AI-powered tool designed to assist project managers in generating Product Requirements Documents (PRDs) and User Stories for their projects. It simplifies the documentation process by leveraging Groq LLM, helping teams structure their project details efficiently. By automating the creation of these essential documents, MiniPM enhances productivity, ensuring well-organized project planning and execution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Requirements\n",
    "\n",
    "This function handles the installation of dependencies from a `requirements.txt` file. It checks if the requirements have already been installed to avoid redundant installations. If not, it attempts to install them using the `pip install -r requirements.txt` command. In case of failure, it retries up to a specified maximum number of times (3 retries in this case) before exiting the program. The function ensures the environment is ready by installing necessary packages to run the application smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function install_requirements() is designed to install the required Python packages listed in the requirements.txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment Setup and API Key Validation\n",
    "\n",
    "The `setup_env()` function loads environment variables from a `.env` file using `load_dotenv()` and retrieves the `GROQ_API_KEY` using `os.getenv()`. If the API key is not found, it prompts the user to set it and terminates the program. If the key is set, it prints a success message confirming the setup, ensuring the necessary credentials are available for the application to proceed securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "    if GROQ_API_KEY is None:\n",
    "        print(\"Please set the GROQ_API_KEY environment variable.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        print(\"GROQ_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup_env() function loads environment variables from a .env file using load_dotenv() to ensure the necessary configurations are available. It then checks for the GROQ_API_KEY in the environment variables, prompting the user to set it if missing. If the key is present, a confirmation message is printed, ensuring the API can be accessed with valid credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: LLM API Interaction and Response Handling\n",
    "\n",
    "This code defines functions for interacting with the Groq LLM API to process user prompts. The `get_groq_client()` function initializes the Groq client with the API key and returns the client instance. The `llm()` function sends a user prompt to the API, retrieves a response, and returns it in the specified format. In case of an error during the process, it catches the exception and returns a structured error response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from groq import Groq\n",
    "import traceback\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "\n",
    "DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "\n",
    "class LLMErrorResponse(BaseModel):\n",
    "    error: str\n",
    "\n",
    "\n",
    "def get_groq_client():\n",
    "    \"\"\"Returns an instance of the Groq class\"\"\"\n",
    "    groq = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    client = instructor.from_groq(groq, mode=instructor.Mode.JSON)\n",
    "    return client\n",
    "\n",
    "\n",
    "def llm(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=\"You are a helpful AI assistant. The user will talk to you and its your job to provide detailed and clear responses.\",\n",
    "    model=DEFAULT_MODEL,\n",
    ") -> Union[BaseModel, LLMErrorResponse]:\n",
    "    \"\"\"Calls LLM API with the given prompt. Defaults to llama-3.3-70b-versatile\"\"\",\n",
    "    try:\n",
    "        client = get_groq_client()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages, model=model, response_model=response_model\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return LLMErrorResponse(error=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Product Requirements Document (PRD) and User Story Generation\n",
    "\n",
    "The **MiniPM** class is an AI-powered tool designed to assist project managers in generating a **Product Requirements Document (PRD)** and **User Stories**. \n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **`get_prd()` Method**:  \n",
    "\n",
    "  Generates a **Product Requirements Document (PRD)** based on a project description. The PRD can be returned in various formats, such as:\n",
    "  \n",
    "  - JSON\n",
    "\n",
    "  - Markdown\n",
    "\n",
    "  - DTO\n",
    "\n",
    "- **`_augment_prd()` Method**:  \n",
    "\n",
    "  Enhances the generated PRD by adding more detailed information to each section, ensuring thoroughness and clarity.\n",
    "\n",
    "- **`get_user_stories()` Method**:  \n",
    "\n",
    "  Generates **User Stories** based on the project details and the generated PRD, helping to break down the project into actionable tasks.\n",
    "\n",
    "### Core Technology:\n",
    "\n",
    "- **Groq LLM Integration**:  \n",
    "\n",
    "  All functionalities are powered by the Groq Large Language Model (LLM), which processes project details and generates the necessary documents and stories with high accuracy.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Product Managers**:  \n",
    "\n",
    "  Helps create well-structured PRDs and User Stories quickly, enhancing productivity.\n",
    "  \n",
    "- **Project Teams**:  \n",
    "\n",
    "  Breaks down complex projects into actionable user stories, ensuring clear objectives and goals.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "The MiniPM class simplifies the process of creating essential project documents like PRDs and User Stories, offering a streamlined solution powered by AI, which is especially useful for product managers and teams looking to enhance their project planning and execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Any\n",
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "class UserStory(BaseModel):\n",
    "    \"\"\"Represents a User Story\"\"\"\n",
    "\n",
    "    title: str\n",
    "    description: str\n",
    "    acceptance_criteria: List[str]\n",
    "    story_points: int\n",
    "\n",
    "\n",
    "class UserStories(BaseModel):\n",
    "    \"\"\"Represents a collection of User Stories\"\"\"\n",
    "\n",
    "    user_stories: List[UserStory]\n",
    "\n",
    "\n",
    "class PRD(BaseModel):\n",
    "    \"\"\"Represents a Product Requirements Document (PRD)\"\"\"\n",
    "\n",
    "    title: str\n",
    "    overview: str\n",
    "    problem_statement: str\n",
    "    business_domains: List[str]\n",
    "    proposed_solution: str\n",
    "    target_audience: str\n",
    "    features: List[str]\n",
    "    constraints: List[str]\n",
    "    assumptions: List[str]\n",
    "    dependencies: List[str]\n",
    "    user_personas: List[str]\n",
    "    conclusion: str\n",
    "\n",
    "\n",
    "class MiniPM:\n",
    "    \"\"\"An AI agent that acts as a Product Manager to help project managers create PRD and User Stories for their projects.\"\"\"\n",
    "\n",
    "    llm: Groq\n",
    "    project_description: str\n",
    "\n",
    "    def __init__(self, project_description: str):\n",
    "        \"\"\"Initializes the MiniPM AI agent with the project description.\"\"\"\n",
    "        self.project_description = project_description\n",
    "        self.llm = get_groq_client()\n",
    "\n",
    "    def get_prd(\n",
    "        self, response_format=\"markdown\", augment=True\n",
    "    ) -> Union[PRD, LLMErrorResponse, Any]:\n",
    "        \"\"\"Gets the Product Requirements Document (PRD) for the project.\"\"\"\n",
    "        system = \"\"\"\n",
    "                You are MiniPM; a Product Manager AI agent. \n",
    "                You help project managers to create a Product Requirements Document (PRD) for their projects.\n",
    "                \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                Provide a Product Requirements Document (PRD) for the project with the following details:\n",
    "                Details: {self.project_description}\n",
    "                \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=PRD, system=system)\n",
    "\n",
    "        if augment:\n",
    "            response = self._augment_prd(response)\n",
    "\n",
    "        if response_format == \"json\":\n",
    "            return response.model_dump_json()\n",
    "        elif response_format == \"markdown\":\n",
    "            response_dict = response.model_dump()\n",
    "            keys = response_dict.keys()\n",
    "            markdown = f\"# Product Requirements Document (PRD) for {response.title}\\n\"\n",
    "            for key in keys:\n",
    "                title = key.title().replace(\"_\", \" \").replace(\"Prd\", \"PRD\")\n",
    "                if key == \"title\":\n",
    "                    title = \"\"\n",
    "                    continue\n",
    "                markdown += f\"## {title}\\n\"\n",
    "                value = response_dict[key]\n",
    "                if type(value) == list:\n",
    "                    value_bullets = [f\"- {item}\" for item in value]\n",
    "                    value = \"\\n\".join(value_bullets)\n",
    "                    markdown += f\"{value}\\n\"\n",
    "                else:\n",
    "                    markdown += f\"{response_dict[key]}\\n\"\n",
    "            return markdown\n",
    "        elif response_format == \"dto\":\n",
    "            return response\n",
    "        raise ValueError(\"Invalid response_format. Use 'json', 'markdown' or 'dto'.\")\n",
    "\n",
    "    def _augment_prd(self, prd: PRD) -> PRD:\n",
    "        \"\"\"Augments the PRD with greater details.\"\"\"\n",
    "        prd_json = prd.model_dump()\n",
    "        print(\"Augmenting PRD: \", prd_json)\n",
    "        keys = prd_json.keys()\n",
    "        for key in keys:\n",
    "            if key == \"title\":\n",
    "                continue\n",
    "            value = prd_json[key]\n",
    "            prompt = f\"Provide more details about the {key} for the project.\"\n",
    "            if type(value) == str:\n",
    "                prompt += f\" Current value: {value}\"\n",
    "                expanded_value = llm(prompt, response_model=str)\n",
    "                prd_json[key] = expanded_value\n",
    "            elif type(value) == list:\n",
    "                expanded_values = []\n",
    "                for item in value:\n",
    "                    prompt += f\" Current value: {item}\"\n",
    "                    expanded_item = llm(prompt, response_model=str)\n",
    "                    expanded_values.append(expanded_item)\n",
    "                prd_json[key] = expanded_values\n",
    "        print(\"Augmented PRD: \", prd_json)\n",
    "        return PRD(**prd_json)\n",
    "\n",
    "    def get_user_stories(self) -> Union[UserStories, LLMErrorResponse]:\n",
    "        \"\"\"Gets the User Stories for the project\"\"\"\n",
    "        system = \"\"\"\n",
    "                You are MiniPM; a Product Manager AI agent. \n",
    "                You help project managers to create User Stories for their projects.\n",
    "                \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                Provide User Stories for the project with the following details:\n",
    "                Details: {self.project_description}\n",
    "                PRD: {self.get_prd()}\n",
    "                \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=UserStories, system=system)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Render and Save Markdown Output\n",
    "\n",
    "The `render_output()` function takes the generated markdown text as input and renders it for display using IPython's `Markdown` class, which allows it to be viewed in a Jupyter notebook interface. The `save_markdown_file()` function takes the markdown content and saves it to a specified file path, enabling the user to persist the generated content as a `.md` file for later use or distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def render_output(markdown: str) -> None:\n",
    "    \"\"\"Renders the generated output file as markdown.\"\"\"\n",
    "    return Markdown(markdown)\n",
    "\n",
    "\n",
    "def save_markdown_file(markdown: str, file_path: str) -> None:\n",
    "    \"\"\"Saves the generated markdown file to the specified path.\"\"\"\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate and Save PRD for Project\n",
    "\n",
    "In this step, the `project_description` is set to a specific project idea (a web application for tracking behavioral changes in aliens). A `MiniPM` instance is created with this project description, and the Product Requirements Document (PRD) is fetched using the `get_prd()` method. The PRD is then saved to a markdown file using `save_markdown_file()`. Finally, the generated PRD markdown content is rendered using `render_output()` for viewing in a Jupyter notebook or similar interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input Parameters: Change these values to get different PRD and User Stories\n",
    "project_description = (\n",
    "    \"A web application for tracking investigating behavioural change in aliens.\"\n",
    ")\n",
    "output_file = \"alien_behavioural_change_prd.md\"\n",
    "augment = True\n",
    "\n",
    "## Create an instance of MiniPM AI agent\n",
    "minipm = MiniPM(project_description)\n",
    "\n",
    "## Get the Product Requirements Document (PRD)\n",
    "prd = minipm.get_prd(response_format=\"markdown\", augment=augment)\n",
    "\n",
    "save_markdown_file(prd, output_file)\n",
    "\n",
    "## Render the PRD as markdown\n",
    "render_output(prd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate User Stories for Project\n",
    "\n",
    "In this step, the `project_description` is set to a new idea (a web application for tracking fitness goals and workouts). An instance of the `MiniPM` AI agent is created with this project description, and the User Stories are fetched using the `get_user_stories()` method. Finally, the generated User Stories are printed in a JSON format using `model_dump_json()` for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input Parameters: Change these values to get different PRD and User Stories\n",
    "project_description = \"A web application for tracking fitness goals and workouts.\"\n",
    "\n",
    "## Create an instance of MiniPM AI agent\n",
    "minipm = MiniPM(project_description)\n",
    "\n",
    "## Get the Product Requirements Document (PRD)\n",
    "user_stories = minipm.get_user_stories()\n",
    "\n",
    "## Render the PRD as markdown\n",
    "print(user_stories.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "**MiniPM** is an AI-powered tool designed to automate and simplify the creation of crucial project documentation, including **Product Requirements Documents (PRDs)** and **User Stories**. By leveraging the **Groq LLM**, MiniPM provides a structured approach to project planning, helping teams save time and focus on execution.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Automated PRD Generation**:  \n",
    "  Quickly generates comprehensive **PRDs**, ensuring that all project details, such as business goals, target audience, and features, are clearly defined.\n",
    "\n",
    "- **User Story Creation**:  \n",
    "  Breaks down complex projects into **actionable User Stories**, making task assignment, prioritization, and tracking more efficient.\n",
    "\n",
    "- **Enhanced Collaboration**:  \n",
    "  Fosters clear communication between team members, stakeholders, and business leaders by producing structured and detailed documentation.\n",
    "\n",
    "- **Time Efficiency**:  \n",
    "  Saves valuable time for project managers by automating documentation creation, allowing them to focus on **strategy** and **execution**.\n",
    "\n",
    "- **Groq LLM Integration**:  \n",
    "  Uses **Groq LLM** to ensure accurate, contextually relevant outputs tailored to the specific needs of each project, guaranteeing high-quality results.\n",
    "\n",
    "\n",
    "MiniPM streamlines the project planning process by automating documentation creation, ensuring well-organized, clear, and actionable project goals. This leads to **higher productivity** and **successful project execution**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
