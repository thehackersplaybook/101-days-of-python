{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Exploratory Data Analyst üìä\n",
    "\n",
    "##### üí° **Research Areas:** Rapid Prototyping, Generative AI, Exploratory Data Analysis, Question-Answering Systems.\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"300px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "An AI data analyst that performs **Exploratory Data Analysis** on a dataset using question-answering model with LLMs.\n",
    "\n",
    "The goal of this prototype is to demonstrate the automation of Data Analytics Activity, highlight use-cases for Pandas AI, and experiment with Question-Answer type models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Dependencies:\n",
    "\n",
    "- `import os`: Imports the os library, which allows you to interact with the operating system. This is useful for accessing environment variables and file paths.\n",
    "\n",
    "- `import pandas as pd`: Imports the Pandas library for data manipulation and analysis. We use Pandas to load datasets (CSV files) into DataFrames, which are tabular data structures.\n",
    "\n",
    "- `from dotenv import load_dotenv`: Imports load_dotenv from the dotenv module. This helps load environment variables from a .env file, ensuring sensitive data like API keys are not hardcoded.\n",
    "\n",
    "- `from typing import List, Union`: Imports List and Union from typing, which are used for type annotations in Python. They help define the expected types of variables and return values.\n",
    "\n",
    "- `from pandasai import SmartDataframe, SmartDatalake`: These are classes from the PandasAI library, which allow you to wrap Pandas DataFrames with AI-driven capabilities (such as querying and analyzing the data using LLMs).\n",
    "\n",
    "- `from pandasai.llm import OpenAI`: Imports the OpenAI API client from PandasAI, which is used to interact with OpenAI models for natural language queries.\n",
    "\n",
    "- `from pydantic import BaseModel`: Imports BaseModel from Pydantic, which is a data validation library. BaseModel is used to define data structures with built-in validation, ensuring that data conforms to the expected format.\n",
    "\n",
    "- `from groq import Groq`: Imports the Groq client, which facilitates interactions with the Groq AI model. It will be used to handle querying and generating responses from the AI model.\n",
    "\n",
    "- `import instructor`: Imports the instructor module, which is used for managing the interface between Groq and the AI model. It helps facilitate communication and query handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Installing Requirements\n",
    "\n",
    "- `requirements_installed = False`: Initializes a boolean flag requirements_installed to False, indicating whether the required dependencies are installed.\n",
    "\n",
    "- `max_retries = 3`: Sets the maximum number of retry attempts for installing the dependencies.\n",
    "\n",
    "- `retries = 0`: Initializes the retries counter to 0.\n",
    "\n",
    "- `def install_requirements():`: Defines the install_requirements function, which checks and installs the required libraries using pip.\n",
    "\n",
    "- `if requirements_installed::`: Checks if the requirements have already been installed. If yes, it prints a message and exits the function.\n",
    "\n",
    "- `install_status = os.system(\"pip install -r requirements.txt\")`: Runs the system command to install the dependencies from requirements.txt. The return status indicates whether the installation was successful.\n",
    "\n",
    "- `if install_status == 0::`: If the installation succeeds (install_status == 0), it sets requirements_installed = True and prints a success message.\n",
    "\n",
    "- `else::`: If the installation fails, it retries up to max_retries. If retries exceed the limit, the program exits with an error.\n",
    "\n",
    "- `install_requirements()`: Calls the install_requirements() function to start the installation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Setting Up Environment Variables\n",
    "\n",
    "- `def setup_env()`: This defines the `setup_env()` function, which is responsible for loading environment variables and checking whether the necessary ones are set. This function ensures that all required variables are available before running the application.\n",
    "\n",
    "- `def check_env(env_var)`: This defines a helper function called check_env to verify if a given environment variable is set. It retrieves the variable's value using os.getenv(env_var).\n",
    "\n",
    "- `value` = os.getenv(env_var): This fetches the value of the specified environment variable using os.getenv. If the variable is not set, it will return `None`.\n",
    "\n",
    "- `if value is None`: If the value returned is None (which means the environment variable is not set), the function prints a message and terminates the program with an error. This ensures the application doesn't run without the necessary environment variables.\n",
    "\n",
    "- `load_dotenv`: This line loads environment variables from a .env file using the dotenv library. The override=True flag ensures that any existing environment variables are overwritten by the values in the .env file, making sure the latest settings are used.\n",
    "\n",
    "- `variables_to_check` = **[\"GROQ_API_KEY\", \"OPENAI_API_KEY\", \"PANDASAI_API_KEY\"]**: Here, a list of required environment variables is defined. These variables are necessary for the proper functioning of the application and are checked later in the process.\n",
    "\n",
    "- `for var in variables_to_check`: This loop iterates through each variable in the variables_to_check list and calls the check_env function to ensure each environment variable is set before proceeding with the application.\n",
    "\n",
    "- `setup_env()`: This line calls the setup_env() function to start the process of checking and loading the required environment variables. It ensures the application has the correct configuration before it starts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "    variables_to_check = [\"GROQ_API_KEY\", \"OPENAI_API_KEY\", \"PANDASAI_API_KEY\"]\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "        \n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Groq Client Setup\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### 1. get_groq_client()\n",
    "\n",
    "- `def get_groq_client():` This function is responsible for initializing the Groq client using the API key provided in the environment variables.\n",
    "\n",
    "- `groq = Groq(api_key=os.getenv(\"GROQ_API_KEY\")):` Creates an instance of the `Groq` class, initializing it with an API key retrieved from the environment variable `GROQ_API_KEY`. If this environment variable is not set, it could lead to errors, so it is important to load and set the key correctly in your environment.\n",
    "\n",
    "- `client = instructor.from_groq(groq, mode=instructor.Mode.JSON):` This line uses the `instructor` module to create a client from the Groq instance. It specifies that the mode of interaction should be `JSON`, meaning that the responses and requests will be in JSON format.\n",
    "\n",
    "- `return client:` The function returns the created client, which will be used to make queries to the Groq API.\n",
    "\n",
    "#### 2. llm()\n",
    "\n",
    "##### Function Definition:\n",
    "\n",
    "- `prompt: str:` The user‚Äôs input that you want the LLM (large language model) to respond to. It's expected to be a string.\n",
    "\n",
    "- `response_model: BaseModel:` Specifies the expected response model for the data returned by the AI. This model must be a subclass of `BaseModel` (likely a Pydantic model).\n",
    "\n",
    "- `system=\"You are a helpful AI assistant...\"`: A default system message that provides context to the AI on how to behave while interacting with the user. The AI is instructed to be a helpful assistant.\n",
    "\n",
    "- `model=DEFAULT_MODEL:` This defines the model that will be used for generating responses. The default model (`DEFAULT_MODEL`) is likely a constant that you should define elsewhere in your code (it could be \"llama-3.3-70b-versatile\" or something else).\n",
    "\n",
    "- `-> Union[BaseModel, LLMErrorResponse]:` This is the return type hint indicating that the function will return either an instance of `BaseModel` (the normal response) or an `LLMErrorResponse` (if there is an error).\n",
    "\n",
    "##### Inside the Function:\n",
    "\n",
    "- `client = get_groq_client():` Calls the previously defined `get_groq_client()` function to obtain the client that interacts with the Groq AI.\n",
    "\n",
    "- `messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]:` This constructs the messages to send to the Groq model. It includes:\n",
    "\n",
    "  - A system message that provides context about the assistant's behavior.\n",
    "  - A user message, which is the actual query or prompt sent by the user.\n",
    "\n",
    "- `response = client.chat.completions.create(...):` This sends the constructed messages to the Groq model and requests a response (completion). The model and `response_model` are used to specify which AI model to use and how to format the response.\n",
    "\n",
    "- `return response:` The function returns the AI's response (it could be structured according to the `response_model`).\n",
    "\n",
    "##### Error Handling:\n",
    "\n",
    "- `except Exception as e:` If an exception is raised during the execution (e.g., network error, invalid API key), it will be caught here.\n",
    "\n",
    "- `traceback.print_exc():` This prints the traceback of the error, providing detailed information about where the exception occurred.\n",
    "\n",
    "- `return LLMErrorResponse(error=str(e)):` If an error occurs, it returns an instance of `LLMErrorResponse`, passing the error message as a string. This helps provide structured error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from groq import Groq\n",
    "import traceback\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def get_groq_client():\n",
    "    \"\"\"Returns an instance of the Groq class\"\"\"\n",
    "    groq = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    client = instructor.from_groq(groq, mode=instructor.Mode.JSON)\n",
    "    return client\n",
    "\n",
    "\n",
    "def llm(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=\"You are a helpful AI assistant. The user will talk to you and its your job to provide detailed and clear responses.\",\n",
    "    model=DEFAULT_MODEL,\n",
    ") -> Union[BaseModel, LLMErrorResponse]:\n",
    "    \"\"\"Calls LLM API with the given prompt. Defaults to llama-3.3-70b-versatile\"\"\",\n",
    "    try:\n",
    "        client = get_groq_client()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages, model=model, response_model=response_model\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return LLMErrorResponse(error=str(e))\n",
    "    \n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Dataset Querying and Analysis System\n",
    "\n",
    "#### 1. Imports and Libraries :\n",
    "\n",
    "- `pandas`: Used for data manipulation, particularly to read and work with CSV files.\n",
    "\n",
    "- `os`: Allows interaction with the operating system (file and directory operations, environment variables).\n",
    "\n",
    "- `typing.List`: Used for type hinting, specifically to indicate that a parameter is a list of items (in this case, strings).\n",
    "\n",
    "- `pandasai`: A library providing AI-enhanced functionalities for dataframes (SmartDataframe) and datalakes (SmartDatalake), which allows interacting with data using language models.\n",
    "\n",
    "- `OpenAI`: Used to interact with OpenAI's language models to analyze and process queries.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. FormattedLLMResponse Class :\n",
    "\n",
    "- A class that defines the structure of the response expected from the language model. It extends `BaseModel` from pydantic, which ensures that the response will be formatted as a response string.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Dataset Class :\n",
    "\n",
    "- Represents a dataset made up of multiple tables (CSV files) located in a specified folder.\n",
    "\n",
    "- It requires a database name and a list of tables (CSV files).\n",
    "\n",
    "- Upon initialization, it checks whether the database and the specified tables exist. If any issues are encountered (e.g., missing files), appropriate exceptions are raised.\n",
    "\n",
    "- Each table is loaded as a pandas DataFrame, and a `SmartDataframe` (an enhanced version of DataFrame) is created for querying the data using AI models.\n",
    "\n",
    "- A `SmartDatalake` is created, which acts as a container for all the `SmartDataframe` objects, enabling cross-table queries using AI.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Methods in Dataset :\n",
    "\n",
    "- `get_table`: This method returns the `SmartDataframe` object for a specified table. If the table does not exist, it raises an exception.\n",
    "\n",
    "- `get_datalake`: Returns the `SmartDatalake` instance, which contains all the `SmartDataframes` for the dataset.\n",
    "\n",
    "- `ask`: This method takes a query (string) and a desired response format (e.g., markdown). It first queries the `SmartDatalake` with the provided query. The results are formatted, and the response is passed to the language model for interpretation. It generates a human-readable summary, insights, and recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Subclasses of Dataset :\n",
    "\n",
    "- `BooksDataset`: A subclass that initializes the `Dataset` class with the \"books\" database and three tables: \"books\", \"ratings\", and \"users\".\n",
    "\n",
    "- `StudentPerformanceDataset`: Another subclass for handling student performance data, initialized with the \"student_performance\" database and the \"student_performance\" table.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Functions for Querying :\n",
    "\n",
    "- `ask_query`: A function that asks a query to a specified dataset. It processes the query using the dataset's `ask` method and formats the result for display.\n",
    "\n",
    "- `ask_query_on_books`: Specifically handles queries for the `BooksDataset` by calling `ask_query` with the \"books\" dataset.\n",
    "\n",
    "- `ask_query_on_student_performance`: Specifically handles queries for the `StudentPerformanceDataset` by calling `ask_query` with the \"student_performance\" dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Business Report Generation :\n",
    "\n",
    "- `BusinessReport` Class: This class is used to define the structure of a business report response, which will be generated from a dataset analysis.\n",
    "\n",
    "- `DataAnalysisAgent`: This agent is responsible for analyzing a dataset by asking predefined analysis questions (e.g., trends, patterns, recommendations). It then compiles the answers and creates a detailed business report using the language model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. Data Analysis Functions :\n",
    "\n",
    "- `data_analysis`: This function uses the `DataAnalysisAgent` to perform an analysis on a given dataset and generates a business report from the analysis results.\n",
    "\n",
    "- `data_analysis_on_books`: A function that runs the analysis on the `BooksDataset` and generates a report.\n",
    "\n",
    "- `data_analysis_on_student_performance`: A function that runs the analysis on the `StudentPerformanceDataset` and generates a report.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts :\n",
    "\n",
    "- **SmartDataframe** and **SmartDatalake**: These are enhanced pandas objects that enable querying data using AI models (like OpenAI's GPT models).\n",
    "\n",
    "- **Query Handling**: Queries are sent to the dataset via the `ask` method, which returns a formatted response with insights, recommendations, or answers based on the data.\n",
    "\n",
    "- **Data Analysis and Business Reporting**: The `DataAnalysisAgent` class performs data analysis by asking a set of predefined questions. It then uses the language model to generate a professional business report summarizing the findings.\n",
    "\n",
    "---\n",
    "\n",
    "### Flow :\n",
    "\n",
    "- **Initialization**: The dataset is loaded from CSV files into memory. The `SmartDataframe` objects are created, and a `SmartDatalake` is set up for AI-based querying.\n",
    "\n",
    "- **Querying**: Users can ask specific queries (e.g., \"What are the top 10 books with the highest average rating?\"), which are answered using the dataset and AI.\n",
    "\n",
    "- **Data Analysis**: The `DataAnalysisAgent` performs deeper analysis by asking predefined questions about the dataset and generating a business report using the AI model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List\n",
    "from pandasai import SmartDataframe, SmartDatalake\n",
    "from pandasai.llm import OpenAI\n",
    "\n",
    "\n",
    "class FormattedLLMResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"Dataset class which contains tables corresponding to CSV files in the base folder\"\"\"\n",
    "\n",
    "    def __init__(self, database: str, tables: List[str]):\n",
    "        llm = OpenAI(api_token=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        if len(tables) == 0:\n",
    "            raise ValueError(\"Tables list cannot be empty.\")\n",
    "\n",
    "        if not os.path.isdir(f\"data/{database}\"):\n",
    "            raise FileNotFoundError(f\"Directory {database} not found.\")\n",
    "\n",
    "        self.base_path = os.path.join(os.getcwd() + \"/data\", database)\n",
    "        self.tables = {}\n",
    "        self.dataframes = {}\n",
    "\n",
    "        for table in tables:\n",
    "            table_file_path = os.path.join(self.base_path, f\"{table}.csv\")\n",
    "            if not os.path.isfile(table_file_path):\n",
    "                print(f\"Table file {table_file_path} not found. Skipping. \")\n",
    "                continue\n",
    "            df = pd.read_csv(table_file_path)\n",
    "            self.tables[table] = SmartDataframe(df)\n",
    "            self.dataframes[table] = pd.read_csv(table_file_path)\n",
    "\n",
    "        print(f\"Loaded {len(self.tables)} tables from {database} dataset.\")\n",
    "\n",
    "        self.datalake = SmartDatalake(\n",
    "            [self.tables[key] for key in self.tables.keys()], config={\"llm\": llm}\n",
    "        )\n",
    "\n",
    "    def get_table(self, table_name: str) -> SmartDataframe:\n",
    "        table = self.tables.get(table_name)\n",
    "        if table is None:\n",
    "            raise ValueError(f\"Table {table_name} not found.\")\n",
    "        return table\n",
    "\n",
    "    def get_datalake(self) -> SmartDatalake:\n",
    "        return self.datalake\n",
    "\n",
    "    def ask(self, query: str, format=\"markdown\") -> str:\n",
    "        print(\"Asking...\")\n",
    "        datalake = self.get_datalake()\n",
    "        df = datalake.chat(query)\n",
    "        print(\"Asked.\")\n",
    "        df_string = \"\"\n",
    "        if type(df) == str:\n",
    "            df_string = df\n",
    "        elif type(df) == pd.DataFrame:\n",
    "            df_top = df.head(10)\n",
    "            df_string = str(df_top)\n",
    "        prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "            Database Result: {df_string}\n",
    "            Instructions:\n",
    "            - Format the data and provide a human readable summary.\n",
    "            - Provide insights and recommendations.\n",
    "            - Don't hallucinate or make up data.\n",
    "            - Provide clear, detailed and concise responses.\n",
    "            f\"The 'response' field should be of format = '{format}'\" if format == \"markdown\" else \"\"\n",
    "        \"\"\"\n",
    "        llm_response = llm(prompt, response_model=FormattedLLMResponse)\n",
    "        return llm_response.response\n",
    "\n",
    "\n",
    "class BooksDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"books\", [\"books\", \"ratings\", \"users\"])\n",
    "\n",
    "\n",
    "class StudentPeformanceDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"student_performance\", [\"student_performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Render Markdown Output\n",
    "\n",
    "#### Function Overview:\n",
    "\n",
    "The `render_output` function is typically used to convert raw Markdown text into a formatted view within a Jupyter notebook, making the output human-readable and visually structured (such as headers, lists, code formatting, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. `from IPython.display import Markdown`:  \n",
    "This imports the Markdown class from the IPython.display module, which allows you to render Markdown-formatted text in IPython environments like Jupyter notebooks.  \n",
    "\n",
    "2. `Function Definition` (render_output):  \n",
    "\n",
    "    - Input: The function takes a single argument, markdown, which is expected to be a string in Markdown format.  \n",
    "\n",
    "    - Output: The function returns the Markdown object initialized with the input string. This object is then rendered by IPython's display mechanism when executed in a Jupyter notebook.  \n",
    "\n",
    "3. `return Markdown` (markdown):  \n",
    "This line creates a Markdown object with the provided markdown text and returns it so it can be displayed in a notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def render_output(markdown: str) -> None:\n",
    "    \"\"\"Renders the generated output file as markdown.\"\"\"\n",
    "    return Markdown(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Querying Dataset and Displaying the Result\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "The `ask_query` function is useful for querying a dataset interactively in a Jupyter notebook. It ensures a clean and readable output by clearing previous outputs and formatting the query and its result properly.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. `from IPython.display import clear_output`:  \n",
    "\n",
    "This imports the `clear_output` function from the IPython.display module, which allows you to clear the output area of a Jupyter notebook. It's often used to refresh the display before showing new content.  \n",
    "\n",
    "2. `Function Definition` (ask_query):  \n",
    "\n",
    "- Input:  \n",
    "    - `query`: A string representing the query you want to ask the dataset.  \n",
    "    - `dataset`: An instance of the `Dataset` class (or its subclass). This dataset contains the data that will be queried.  \n",
    "\n",
    "- Output:  \n",
    "    - The function doesn't return anything directly but uses the `render_output` function to display the formatted result in the notebook.  \n",
    "\n",
    "3. `response = dataset.ask(query)`:  \n",
    "\n",
    "This calls the ask method of the provided dataset and passes the query. The result of the query is stored in response.  \n",
    "\n",
    "4. `clear_output()`:  \n",
    "\n",
    "This clears the notebook's output area to ensure that previous outputs do not clutter the screen before displaying the new response.  \n",
    "\n",
    "5. `response = f\"### Query: {query}\\n{response}\"`:  \n",
    "\n",
    "This formats the response by adding a heading (`### Query: {query}`) and the actual response from the query. The query is displayed above the result for clarity.  \n",
    "\n",
    "6. `return render_output(response)`:  \n",
    "\n",
    "The formatted response is passed to the `render_output` function, which is responsible for rendering the Markdown-formatted text for display in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def ask_query(query: str, dataset: Dataset) -> None:\n",
    "    \"\"\"Asks the given query to the dataset and returns the response.\"\"\"\n",
    "    response = dataset.ask(query)\n",
    "    clear_output()\n",
    "    response = f\"### Query: {query}\\n{response}\"\n",
    "    return render_output(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Query Execution on Books and Student Performance\n",
    "\n",
    "`ask_query_on_books` & `ask_query_on_student_performance`:\n",
    "\n",
    "- Specialized functions that initialize the respective dataset objects and query the data using the ask_query method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_query_on_books(query: str) -> None:\n",
    "    \"\"\"Main function to run the script\"\"\"\n",
    "    books_dataset = BooksDataset()\n",
    "    return ask_query(query, books_dataset)\n",
    "\n",
    "\n",
    "def ask_query_on_student_performance(query: str) -> None:\n",
    "    \"\"\"Main function to run the script\"\"\"\n",
    "    student_performance_dataset = StudentPeformanceDataset()\n",
    "    return ask_query(query, student_performance_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Running Queries\n",
    "- Sample Queries:\n",
    "\n",
    "    - Queries for top 10 books with highest average ratings and top 10 students with highest average grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the top 10 books with the highest average rating?\"\n",
    "\n",
    "ask_query_on_books(query)\n",
    "\n",
    "query = \"What are the top 10 students with the highest average grades?\"\n",
    "\n",
    "ask_query_on_student_performance(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: DataAnalysisAgent Class\n",
    "\n",
    "- Handles data analysis by asking a series of predefined questions and aggregating the answers into a business report.\n",
    "- `analyse` method: Asks predefined questions, compiles answers, and generates a report using LLM.\n",
    "\n",
    "- `get_analysis` method: Returns the analysis, either generated or cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class BusinessReport(BaseModel):\n",
    "    markdown: str\n",
    "\n",
    "\n",
    "class DataAnalysisAgent:\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.dataset = dataset\n",
    "        self.analysis = None\n",
    "\n",
    "    def analyse(self) -> str:\n",
    "        # instruction = \"Respond in a human readable format. Don't provide graphs or execute code.\"\n",
    "        analysis_questions = [\n",
    "            \"What are the key insights from the dataset?\",\n",
    "            \"What are the key trends in the dataset?\",\n",
    "            \"What are the key patterns in the dataset?\",\n",
    "            \"What are the key anomalies in the dataset?\",\n",
    "            \"What are the key recommendations based on the dataset?\",\n",
    "        ]\n",
    "        self.analysis = \"\"\n",
    "        for question in analysis_questions:\n",
    "            try:\n",
    "                answer = self.dataset.ask(question)\n",
    "                self.analysis += f\"\\n### üëâüèΩ {question}\\nüí° {answer}\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to ask question: {question}\")\n",
    "                # print(e)\n",
    "                continue\n",
    "        print(\"Preliminary analysis completed. Generating business report...\")\n",
    "        prompt = f\"\"\"\n",
    "            Understand the provided analysis for the dataset and provide a neatly formatted report.\n",
    "            This report should be professional, detailed and ideal for business stakeholders.\n",
    "            The report should include all facts in the preliminary analysis and provide additional insights.\n",
    "            The 'response' field should be of format = 'markdown'\n",
    "\n",
    "            Preliminary Analysis: {self.analysis}\n",
    "        \"\"\"\n",
    "        llm_response = llm(prompt=prompt, response_model=BusinessReport)\n",
    "        if not llm_response:\n",
    "            print(\"Failed to get response from LLM.\")\n",
    "            return self.analysis\n",
    "        self.analysis = llm_response.markdown\n",
    "        print(\"Business report generated.\")\n",
    "        return self.analysis\n",
    "\n",
    "    def get_analysis(self) -> str:\n",
    "        if self.analysis is None:\n",
    "            return self.analyse()\n",
    "        return self.analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Data Analysis Execution\n",
    "- `data_analysis_on_books` & `data_analysis_on_student_performance`:\n",
    "\n",
    "    - Calls the DataAnalysisAgent on the respective datasets (Books and Student Performance) and renders the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Markdown\n",
    "\n",
    "\n",
    "def data_analysis(dataset: Dataset) -> Markdown:\n",
    "    agent = DataAnalysisAgent(dataset)\n",
    "    analysis = agent.analyse()\n",
    "    clear_output()\n",
    "    return render_output(analysis)\n",
    "\n",
    "\n",
    "def data_analysis_on_books() -> Markdown:\n",
    "    books_dataset = BooksDataset()\n",
    "    return data_analysis(books_dataset)\n",
    "\n",
    "\n",
    "def data_analysis_on_student_performance() -> Markdown:\n",
    "    student_performance_dataset = StudentPeformanceDataset()\n",
    "    return data_analysis(student_performance_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Triggering the Data Analysis:\n",
    "\n",
    "- Calls data analysis on both the student performance and books datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis_on_student_performance()\n",
    "data_analysis_on_books()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
