{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://img.utdstc.com/icon/716/88f/71688f4905ece2a5ee744eaf351ec21bd51491e02025ea4a68501cc93d847e5c:200\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "This project implements a sentiment analysis model for Twitter data using the BERT transformer model. The pipeline includes data preprocessing, tokenization, model training, and evaluation. It reads a dataset of Twitter posts, performs necessary preprocessing (such as encoding sentiments), and then trains a sentiment classification model using BERT. The model is capable of classifying sentiments into three categories: Positive, Negative, and Neutral. The code also includes functions for installing dependencies, setting up environment variables, and saving the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "- The function install_requirements installs the necessary packages from the requirements.txt file.\n",
    "\n",
    "- It ensures packages are installed, retrying a set number of times if the installation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Environment Variables\n",
    "\n",
    "- Loads environment variables from a .env file using dotenv and checks if specific variables are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = []\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Read Dataset\n",
    "\n",
    "- Reads a CSV file containing Twitter sentiment analysis data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_dataset():\n",
    "    \"\"\"Reads the dataset\"\"\"\n",
    "    dataset = pd.read_csv(\"data/twitter_sentiment_analysis/twitter.csv\")\n",
    "    return dataset\n",
    "dataset = read_dataset()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training and Testing Data\n",
    "\n",
    "This step processes the dataset by reading it, splitting it into training and testing sets, encoding sentiment labels as numbers, and applying transformations like scaling for numeric data and one-hot encoding for categorical data. The data is then converted into PyTorch tensors for use in training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def get_train_test_data():\n",
    "    \"\"\"Prepare the training and testing data.\"\"\"\n",
    "    dataset = read_dataset()\n",
    "    target_column = \"sentiment\"\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    \n",
    "    # Encode target labels into numerical values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Separate columns into categorical and numeric\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "    numeric_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "    \n",
    "    # Preprocessing the data (Scaling numeric columns and One-Hot Encoding categorical columns)\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"num\", StandardScaler(), numeric_cols),\n",
    "                      (\"cat\", OneHotEncoder(sparse_output=False), categorical_cols)])\n",
    "    \n",
    "    # Split the dataset into training and testing data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Apply preprocessing and convert to PyTorch tensors\n",
    "    X_train = torch.tensor(preprocessor.fit_transform(X_train), dtype=torch.float32)\n",
    "    X_test = torch.tensor(preprocessor.transform(X_test), dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "dataset_path = \"data/twitter_sentiment_analysis/twitter.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocessing and Dataset Class for Sentiment Analysis using BERT\n",
    "\n",
    "This step involves preprocessing the dataset for a BERT-based sentiment analysis model. It handles missing tweet text, converts sentiment labels to numerical values, and creates a custom `TwitterDataset` class for PyTorch. \n",
    "\n",
    "The class tokenizes tweets using a BERT tokenizer, prepares input data (token IDs, attention masks), and returns the sentiment label, making the data ready for training with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load your dataset\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Ensure the 'content' column has only strings and handle missing values\n",
    "dataset[\"content\"] = dataset[\"content\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Preprocessing sentiment to numerical labels\n",
    "def sentiment_to_label(sentiment):\n",
    "    return {\"Positive\": 1, \"Negative\": 0, \"Neutral\": 2}.get(\n",
    "        sentiment, 2\n",
    "    )  # Default to Neutral if not found\n",
    "\n",
    "dataset[\"sentiment_label\"] = dataset[\"sentiment\"].apply(sentiment_to_label)\n",
    "\n",
    "# Dataset Class\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tweet = self.data.iloc[index][\"content\"]\n",
    "        sentiment = self.data.iloc[index][\"sentiment_label\"]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            tweet,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(sentiment, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model and Training Setup\n",
    "\n",
    "This step sets up the model and training configuration for fine-tuning BERT on sentiment classification. Key components include defining hyperparameters (batch size, learning rate, epochs), using the Hugging Face BertTokenizer, splitting the dataset into training and validation, and creating a custom `SentimentClassifier` model with a BERT layer, dropout, and a linear output layer for sentiment prediction. The training uses Cross-Entropy Loss and the AdamW optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)  # Shuffle dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_data = dataset[:train_size]\n",
    "val_data = dataset[train_size:]\n",
    "\n",
    "train_dataset = TwitterDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TwitterDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = SentimentClassifier(n_classes=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train and Evaluate the Model\n",
    "\n",
    "This step involves training and evaluating the sentiment analysis model. It defines two key functions:\n",
    "\n",
    "- `train_epoch`: Trains the model for one epoch, computes the loss and accuracy, and updates the model's weights.\n",
    "\n",
    "- `eval_model`: Evaluates the model on the validation set, calculating loss and accuracy to assess performance.\n",
    "\n",
    "The **training loop** runs for multiple epochs, training and evaluating the model in each epoch. After training, the model's state is saved for future use. This structure helps track performance and improve the model iteratively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    i = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation Function\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), \"sentiment_model.pth\")\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This sentiment analysis application uses a deep learning model based on BERT (Bidirectional Encoder Representations from Transformers) to classify tweets into three sentiment categories: Positive, Negative, and Neutral. The app follows a systematic approach to preprocess the data, tokenize the text using BERT's tokenizer, and train a model to predict sentiment.\n",
    "\n",
    "- Key highlights of the app include:\n",
    "\n",
    "    - `Data Preprocessing` : It handles missing values and encodes sentiment labels into numerical format for training.\n",
    "\n",
    "    - `Model Architecture`: The app uses a pre-trained BERT model, which has been fine-tuned to adapt to the sentiment classification task. A dropout layer is applied to prevent overfitting, and a fully connected layer is used for output classification.\n",
    "\n",
    "    - `Training and Evaluation`: The app includes functions for training and evaluating the model across multiple epochs, monitoring the loss and accuracy on both training and validation datasets.\n",
    "\n",
    "    - `Performance Monitoring`: The training loop prints the loss and accuracy after each epoch, allowing for easy tracking of model performance.\n",
    "\n",
    "    - `Model Saving`: After training, the model is saved to a file, allowing for future predictions or model loading without retraining.\n",
    "\n",
    "Overall, this application provides a robust framework for sentiment analysis of tweets, leveraging the power of BERT to achieve high accuracy and efficiency. It can be further extended to perform sentiment analysis on larger datasets or integrated into a web application for real-time tweet sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! ðŸŒ\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
