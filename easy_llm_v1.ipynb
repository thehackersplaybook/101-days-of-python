{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EasyLLM: Simplified OpenAI API Abstraction\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://www.freeiconspng.com/thumbs/calculator-icon/calculator-icon-26.jpg\"> \n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "This app provides an easy-to-use abstraction for the OpenAI API. It simplifies generating text and objects using OpenAI's GPT models with customizable prompts, making it ideal for tasks like content creation, generating structured data, and more. The app includes setup for dependencies, environment variables, and example workflows for generating text and structured responses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Boilerplate Setup\n",
    "\n",
    "This step sets up the boilerplate code for the project. It includes:\n",
    "\n",
    "- ### Import Statements:\n",
    "\n",
    "    - `os`: Used for running shell commands and accessing environment variables.\n",
    "\n",
    "    - `load_dotenv`: Loads environment variables from a `.env` file into the system for easy configuration.\n",
    "\n",
    "    - `clear_output`: Clears the notebook's output to keep it clean after successful setup.\n",
    "\n",
    "\n",
    "\n",
    "- ### Global Variables:\n",
    "\n",
    "    - `requirements_installed`: Tracks whether dependencies are already installed.\n",
    "    \n",
    "    - `max_retries`: Limits how many times the code will retry installing dependencies in case of failure.\n",
    "    \n",
    "    - `REQUIRED_ENV_VARS`: Specifies the environment variables that must be present.\n",
    "\n",
    "\n",
    "\n",
    "- ### `install_requirements` Function:\n",
    "\n",
    "    - Uses the `os.system` command to run `pip install -r requirements.txt`.\n",
    "\n",
    "    - If the installation fails, it retries up to `max_retries`.\n",
    "\n",
    "    - If retries are exhausted, it exits the program with a failure code.\n",
    "\n",
    "\n",
    "\n",
    "- ### `setup_env` Function:\n",
    "\n",
    "    - Loads environment variables from `.env`.\n",
    "\n",
    "    - Verifies the presence of each required variable using the `check_env` function.\n",
    "\n",
    "    - Exits the program if any required environment variable is missing.\n",
    "\n",
    "\n",
    "\n",
    "- ### Execution:\n",
    "\n",
    "    - Calls `install_requirements` to install dependencies.\n",
    "\n",
    "    - Calls `setup_env` to validate the environment.\n",
    "\n",
    "    - Clears the output and confirms the setup is complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"üöÄ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Defining the EasyLLM Class:\n",
    "\n",
    "### **Purpose**  \n",
    "The `EasyLLM` class provides an abstraction layer for interacting with OpenAI's API. It simplifies the process of:  \n",
    "\n",
    "- Generating text responses.  \n",
    "- Parsing structured responses into objects using Pydantic models.  \n",
    "- Switching between OpenAI models.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Detailed Code Explanation**\n",
    "\n",
    "#### **Class-Level Defaults**\n",
    "\n",
    "- `DEFAULT_OPENAI_MODEL`: Sets the default OpenAI model to `gpt-4o-mini`.  \n",
    "\n",
    "- `DEFAULT_SYSTEM_PROMPT`: Provides a default system instruction for the AI assistant.  \n",
    "\n",
    "- `DEFAULT_TEMPERATURE` and `DEFAULT_MAX_TOKENS`: Control the response style (creativity) and length.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **`__init__` Method**  \n",
    "\n",
    "- Accepts `api_key`, `model`, and logging preferences (`verbose` and `debug`).  \n",
    "\n",
    "- Initializes the OpenAI client with the provided API key.  \n",
    "\n",
    "- Prints initialization messages if `verbose` or `debug` mode is enabled.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **`generate_text` Method**  \n",
    "\n",
    "- Takes a prompt and optional parameters like `system_prompt`, `temperature`, and `token_limit`.  \n",
    "\n",
    "- Sends a request to OpenAI's `chat.completions.create` API.  \n",
    "\n",
    "- Logs detailed request parameters if `debug` is enabled.  \n",
    "\n",
    "- Extracts and returns the content from the API's response.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Error Handling**  \n",
    "\n",
    "- Catches exceptions, prints error messages, and optionally logs stack traces in `debug` mode.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **`generate_object` Method**  \n",
    "\n",
    "- Extends `generate_text` to parse responses into structured objects using Pydantic models.  \n",
    "\n",
    "- Takes a `response_model` (a Pydantic model class) to define the expected response format.  \n",
    "\n",
    "- Returns the parsed object if successful.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Management**  \n",
    "\n",
    "- **`get_model`**: Returns the currently active model.  \n",
    "\n",
    "- **`set_model`**: Changes the model used for generating responses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! üöÄ\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! ‚ö°Ô∏è\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate a Poem Using the EasyLLM Class\n",
    "\n",
    "### **Purpose**  \n",
    "\n",
    "This part demonstrates how to use the `EasyLLM` class to generate a simple poem for a specific audience (children aged 15-18) using the `generate_text` method.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Breakdown**\n",
    "\n",
    "#### **Imports:**\n",
    "\n",
    "- `pprint (pp)`: Pretty prints Python data structures, which helps in making the output more readable.  \n",
    "\n",
    "- `clear_output`: Clears the notebook output area after executing the code (often used in Jupyter notebooks).  \n",
    "\n",
    "---\n",
    "\n",
    "**Create EasyLLM Instance:** \n",
    "``` bash \n",
    "llm = EasyLLM() \n",
    "```\n",
    "**Prompt Generation:**\n",
    "``` bash\n",
    "prompt = \"Generate a simple poem on learning for children between age 15 to 18.\"\n",
    "```\n",
    "\n",
    "The prompt provided to the model asks it to generate a poem tailored for children aged 15 to 18 about learning. This prompt is essential for guiding the model's output\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "response = llm.generate_text(prompt)\n",
    "```\n",
    "This invokes the generate_text method, passing the prompt. The method will interact with OpenAI's API and return the generated text (in this case, a poem). The response is stored in the variable response.\n",
    "\n",
    "  - clear_output(): Clears any previous outputs in the notebook to keep things neat.\n",
    "\n",
    "  - pp(f\"Prompt: {prompt}\"): Pretty prints the original prompt.\n",
    "\n",
    "  - pp(response): Pretty prints the generated poem. This makes it easier to read and understand the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "llm = EasyLLM()\n",
    "\n",
    "prompt = \"Generate a simple poem on learning for children between age 15 to 18.\"\n",
    "\n",
    "response = llm.generate_text(prompt)\n",
    "clear_output()\n",
    "pp(f\"Prompt: {prompt}\")\n",
    "pp(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate a Quote Using Structured Data (Object)\n",
    "\n",
    "### **Purpose**  \n",
    "\n",
    "This step demonstrates how to generate a structured object using OpenAI‚Äôs API, where the response is parsed into a predefined Pydantic model. The `generate_object` method is used here.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Breakdown**\n",
    "\n",
    "#### **Imports:**\n",
    "\n",
    "- Same as before, `pprint` and `clear_output` are imported for output formatting.  \n",
    "\n",
    "- `BaseModel` (from Pydantic): Used to define response models that are structured and validated.  \n",
    "\n",
    "- `Enum`: Defines a set of possible values for certain fields (e.g., Emotion and Sentiment).  \n",
    "\n",
    "- `List`: Allows the use of lists in type annotations.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **Define the Response Model (QuoteResponse):**\n",
    "\n",
    "##### **Enum Classes:**\n",
    "\n",
    "- `Emotion`: This defines possible emotional states associated with a quote (e.g., `HAPPY`, `SAD`, `NEUTRAL`).  \n",
    "\n",
    "- `Sentiment`: This defines the possible sentiments (e.g., `POSITIVE`, `NEGATIVE`, `NEUTRAL`) that the generated quote might have.  \n",
    "\n",
    "\n",
    "##### **Tag Class:**\n",
    "\n",
    "- A `Tag` class is defined, which has two fields:\n",
    "\n",
    "  - `key`: A string identifier for the tag.\n",
    "\n",
    "  - `name`: A descriptive name for the tag.\n",
    "\n",
    "\n",
    "##### **QuoteResponse Class:**\n",
    "\n",
    "This class models the response that the `generate_object` method will return. It consists of:\n",
    "\n",
    "- `quote`: The generated quote (a string).\n",
    "\n",
    "- `meaning`: Explanation of the quote (a string).\n",
    "\n",
    "- `fictious_author`: A made-up author name for the quote (a string).\n",
    "\n",
    "- `emotion`: A list of emotions related to the quote.\n",
    "\n",
    "- `sentiment`: Sentiment of the quote.\n",
    "\n",
    "- `target_audience`: List of target audiences for the quote (e.g., \"students\", \"teachers\").\n",
    "\n",
    "- `tags`: A list of tags that categorize the quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from IPython.display import clear_output\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Emotion(str, Enum):\n",
    "    HAPPY = \"happy\"\n",
    "    SAD = \"sad\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "\n",
    "\n",
    "class Sentiment(str, Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "\n",
    "\n",
    "class Tag(BaseModel):\n",
    "    key: str\n",
    "    name: str\n",
    "\n",
    "\n",
    "class QuoteResponse(BaseModel):\n",
    "    quote: str\n",
    "    meaning: str\n",
    "    fictious_author: str\n",
    "    emotion: List[Emotion]\n",
    "    sentiment: Sentiment\n",
    "    target_audience: List[str]\n",
    "    tags: List[Tag]\n",
    "\n",
    "\n",
    "llm = EasyLLM()\n",
    "\n",
    "prompt = \"Generate a unique quote on time.\"\n",
    "\n",
    "response = llm.generate_object(prompt, response_model=QuoteResponse)\n",
    "clear_output()\n",
    "pp(f\"Prompt: {prompt}\")\n",
    "pp(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "This app demonstrates a powerful and flexible way to interact with OpenAI's GPT-4 model using a simple abstraction, EasyLLM. By encapsulating the complexity of direct API calls, this app allows users to easily generate dynamic text responses (like poems or quotes) while also offering the ability to handle more structured outputs (like parsed objects) using Pydantic models.\n",
    "\n",
    "### Key takeaways from this app include:\n",
    "\n",
    "- `Simplified API Interaction`: The EasyLLM class abstracts the complexity of interacting with OpenAI‚Äôs API, enabling easier use of language models for text generation and structured object creation.\n",
    "\n",
    "- `Customizable Prompts`: Users can tailor the prompts to generate specific types of content (e.g., poems, quotes) based on their needs.\n",
    "\n",
    "- `Structured Responses`: For use cases that require structured data, the app demonstrates how to generate and parse complex responses using Pydantic models. This ensures that the output is both predictable and validated.\n",
    "\n",
    "- `Error Handling and Debugging`: With built-in error handling and debugging options, users can troubleshoot and improve the app‚Äôs stability.\n",
    "\n",
    "This app serves as a solid foundation for developing more advanced AI-powered applications, including those that require natural language processing, sentiment analysis, and content generation. By leveraging the OpenAI API with clear abstractions and simple configurations, this app allows for rapid prototyping of intelligent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
