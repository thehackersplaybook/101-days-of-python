{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"ðŸš€ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! ðŸš€\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! âš¡ï¸\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. ðŸŽ‰\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. ðŸŽ‰\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "llm = EasyLLM()\n",
    "\n",
    "prompt = \"Generate a simple poem on learning for children between age 15 to 18.\"\n",
    "\n",
    "response = llm.generate_text(prompt)\n",
    "clear_output()\n",
    "pp(f\"Prompt: {prompt}\")\n",
    "pp(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from IPython.display import clear_output\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Emotion(str, Enum):\n",
    "    HAPPY = \"happy\"\n",
    "    SAD = \"sad\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "\n",
    "\n",
    "class Sentiment(str, Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "\n",
    "\n",
    "class Tag(BaseModel):\n",
    "    key: str\n",
    "    name: str\n",
    "\n",
    "\n",
    "class QuoteResponse(BaseModel):\n",
    "    quote: str\n",
    "    meaning: str\n",
    "    fictious_author: str\n",
    "    emotion: List[Emotion]\n",
    "    sentiment: Sentiment\n",
    "    target_audience: List[str]\n",
    "    tags: List[Tag]\n",
    "\n",
    "\n",
    "llm = EasyLLM()\n",
    "\n",
    "prompt = \"Generate a unique quote on time.\"\n",
    "\n",
    "response = llm.generate_object(prompt, response_model=QuoteResponse)\n",
    "clear_output()\n",
    "pp(f\"Prompt: {prompt}\")\n",
    "pp(response.model_dump_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
