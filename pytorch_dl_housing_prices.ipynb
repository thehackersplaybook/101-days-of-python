{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Price Prediction Using Neural Networks\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "##  Description:\n",
    "\n",
    "This application utilizes a deep learning model to predict housing prices based on various features using a neural network. It includes preprocessing, model training, and evaluation through multiple experimental setups to optimize the prediction accuracy. The app leverages synthetic data and different hyperparameter configurations for improved model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Requirements\n",
    "\n",
    "This step initializes flags and variables to check whether the required dependencies are installed. The max_retries and retries variables handle retry attempts if installation fails.\n",
    "\n",
    "This function checks if the necessary Python dependencies (from requirements.txt) are installed, and if not, it attempts to install them. If installation fails, it retries up to 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    " This step loads environment variables from a .env file. It checks whether specific environment variables are set and warns the user if they are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = []\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load, Preprocess, and Train Housing Price Prediction Model\n",
    "\n",
    "This code defines a housing price prediction model using a neural network in PyTorch. It loads a CSV dataset, preprocesses it (scaling numeric features and one-hot encoding categorical ones), and splits it into training and test sets.\n",
    "\n",
    " A custom model class builds the neural network architecture dynamically based on the configuration (e.g., number of layers, activation functions). \n",
    " \n",
    " The model is trained using the Adam optimizer and MSE loss. After training, it can make predictions and evaluate performance metrics such as MSE, RMSE, MAPE, and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "# Adjust the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "class HousingPriceDataset:\n",
    "    \"\"\"Class to load and preprocess the housing dataset.\"\"\"\n",
    "    def __init__(self, csv_path, target_column, scale_factor=1_000_000):\n",
    "        # Load the dataset\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "\n",
    "        # Separate features and target\n",
    "        self.X = self.data.drop(columns=[target_column])\n",
    "        self.y = self.data[target_column] / scale_factor  # Scale target\n",
    "\n",
    "        # Identify categorical and numeric columns\n",
    "        categorical_cols = self.X.select_dtypes(include=['object']).columns\n",
    "        numeric_cols = self.X.select_dtypes(include=['number']).columns\n",
    "\n",
    "        # Preprocess the data\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_cols),\n",
    "                ('cat', OneHotEncoder(), categorical_cols)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Fit and transform the training data, transform the testing data\n",
    "        self.X_train = torch.tensor(preprocessor.fit_transform(X_train), dtype=torch.float32)\n",
    "        self.X_test = torch.tensor(preprocessor.transform(X_test), dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "        self.y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def get_input_size(self):\n",
    "        return self.X_train.shape[1]\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.X_train, self.y_train\n",
    "\n",
    "class HousingPriceModel(nn.Module):\n",
    "    \"\"\"Class to define the neural network model for predicting housing prices.\"\"\"\n",
    "    def __init__(self, dataset: HousingPriceDataset, training_config=None):\n",
    "        if training_config is None:\n",
    "            training_config = {\n",
    "                \"epochs\": 500,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"dropout_rate\": 0.5,\n",
    "                \"l2_lambda\": 0.01,\n",
    "                \"layers\": [64, 32],\n",
    "                \"activation\": \"ReLU\"\n",
    "            }\n",
    "        input_size = dataset.get_input_size()\n",
    "        self.X_train, self.y_train = dataset.get_train_data()\n",
    "        self.training_config = training_config\n",
    "        super(HousingPriceModel, self).__init__()\n",
    "\n",
    "        # Build the model dynamically based on config\n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "        for size in training_config[\"layers\"]:\n",
    "            layers.append(nn.Linear(previous_size, size))\n",
    "            if training_config[\"activation\"] == \"ReLU\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif training_config[\"activation\"] == \"Tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            elif training_config[\"activation\"] == \"Sigmoid\":\n",
    "                layers.append(nn.Sigmoid())\n",
    "            layers.append(nn.Dropout(training_config[\"dropout_rate\"]))\n",
    "            previous_size = size\n",
    "        layers.append(nn.Linear(previous_size, 1))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.cached_model = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def set_training_config(self, config):\n",
    "        self.training_config = deepcopy(config)\n",
    "\n",
    "    def get_training_config(self):\n",
    "        return deepcopy(self.training_config)\n",
    "\n",
    "    def train_model(self):\n",
    "        training_config = self.get_training_config()\n",
    "        epochs = training_config[\"epochs\"]\n",
    "        learning_rate = training_config[\"learning_rate\"]\n",
    "        l2_lambda = training_config[\"l2_lambda\"]\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            outputs = self(self.X_train)\n",
    "            loss = criterion(outputs, self.y_train)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss every 50 epochs\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Cache the trained model\n",
    "        self.cached_model = self.state_dict()\n",
    "\n",
    "    def _assert_model_state(self):\n",
    "        if self.cached_model is None:\n",
    "            print(\"Model is not trained. Training the model now...\")\n",
    "            self.train_model()\n",
    "\n",
    "    def predict_price(self, X):\n",
    "        self._assert_model_state()\n",
    "        self.eval() \n",
    "        with torch.no_grad():\n",
    "            prediction = self(X)\n",
    "        return prediction.item()\n",
    "\n",
    "    def batch_predict(self, X, y=None):\n",
    "        self._assert_model_state()\n",
    "        self.eval()  \n",
    "        metrics = {\n",
    "            \"loss\": None,\n",
    "            \"rms\": None,\n",
    "            \"mape\": None,\n",
    "            \"mae\": None\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            predictions = self(X)\n",
    "            if y is not None:\n",
    "                loss = nn.MSELoss()(predictions, y).item()\n",
    "                rms = torch.sqrt(torch.tensor(loss)).item()\n",
    "                mape = torch.mean(torch.abs((predictions - y) / y) * 100).item()\n",
    "\n",
    "                metrics[\"loss\"] = loss\n",
    "                metrics[\"rms\"] = rms\n",
    "                metrics[\"mape\"] = mape\n",
    "                metrics[\"mae\"] = nn.L1Loss()(predictions, y).item()\n",
    "        return predictions, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Experiment and Evaluate Model Performance\n",
    "\n",
    "This step defines the run_experiment function, which:\n",
    "\n",
    "\n",
    "- Loads the housing price dataset.\n",
    "\n",
    "- Initializes and trains the HousingPriceModel using the provided training configuration.\n",
    "\n",
    "- Evaluates the model on test data and calculates performance metrics such as MSE, RMSE, MAPE, and MAE.\n",
    "\n",
    "- Displays the metrics and compares sample predictions with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    csv_path,\n",
    "    target_column,\n",
    "    training_config={\n",
    "        \"epochs\": 500,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64, 32],\n",
    "        \"activation\": \"ReLU\",\n",
    "    },\n",
    "):\n",
    "    \"\"\"Method to run the experiment\"\"\"\n",
    "    dataset = HousingPriceDataset(csv_path, target_column)\n",
    "    model = HousingPriceModel(dataset, training_config)\n",
    "    model.train_model()\n",
    "    predictions, metrics = model.batch_predict(dataset.X_test, dataset.y_test)\n",
    "    clear_output()\n",
    "\n",
    "    print(\"METRICS:\")\n",
    "    print(f\"MSE: {metrics['loss']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rms']:.4f}\")\n",
    "    print(f\"MAPE: {metrics['mape']:.4f}%\")\n",
    "    print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "\n",
    "    # Print sample predictions\n",
    "    print(\"Sample predictions vs actual values:\")\n",
    "    for i in range(5):\n",
    "        print(\n",
    "            f\"Predicted: {predictions[i].item():.2f}, Actual: {dataset.y_test[i].item():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Basic Experiment with Initial Training Configuration\n",
    "\n",
    "This experiment uses a basic training configuration with the following settings:\n",
    "\n",
    "- 1000 epochs\n",
    "\n",
    "- Learning rate: 0.01\n",
    "\n",
    "- Dropout rate: 0.5\n",
    "\n",
    "- L2 regularization: 0.01\n",
    "\n",
    "- Two hidden layers (128 and 64 neurons)\n",
    "\n",
    "- ReLU activation function\n",
    "\n",
    "The housing price dataset is loaded from data/housing_prices/housing_prices.csv, and the target column is \"price\". The model is trained, and the results, including performance metrics and sample predictions, are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Basic stuff.\n",
    "\n",
    "training_config = {\n",
    "        \"epochs\": 1000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64],\n",
    "        \"activation\": \"ReLU\"\n",
    "    }\n",
    "\n",
    "csv_path = \"data/housing_prices/housing_prices.csv\"\n",
    "target_column = \"price\"  \n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Optimized Experiment with 1000 Epochs and 3 Layers\n",
    "\n",
    "This experiment uses a refined configuration with the following settings:\n",
    "\n",
    "\n",
    "- 1000 epochs\n",
    "\n",
    "- Learning rate: 0.01\n",
    "\n",
    "- Dropout rate: 0.5\n",
    "\n",
    "- L2 regularization: 0.01\n",
    "\n",
    "- Three hidden layers (128, 64, and 32 neurons)\n",
    "\n",
    "- ReLU activation function\n",
    "\n",
    "It provides better performance as compared to the previous experiment, delivering improved metrics for housing price predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: 1000 epochs, 3 layers, ReLU activation\n",
    "# This works the best. \n",
    "\n",
    "training_config = {\n",
    "        \"epochs\": 1000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64, 32],\n",
    "        \"activation\": \"ReLU\"\n",
    "    }\n",
    "\n",
    "csv_path = \"data/housing_prices/housing_prices.csv\"\n",
    "target_column = \"price\"  \n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Experiment with Increased Epochs and More Layers\n",
    "\n",
    "In this experiment, the following changes were made:\n",
    "\n",
    "- 5000 epochs (increased training duration)\n",
    "\n",
    "- Learning rate: 0.05 (higher learning rate)\n",
    "\n",
    "- Dropout rate: 0.5 (kept the same)\n",
    "\n",
    "- L2 regularization: 0.01 (kept the same)\n",
    "\n",
    "- Five hidden layers with varying sizes: [128, 64, 32, 64, 128]\n",
    "\n",
    "- ReLU activation function\n",
    "\n",
    "This setup explores the effects of longer training time and a deeper network, and it may help refine model performance, though further adjustments might be needed based on metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: 5x the epochs and add more hidden layers\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": 5000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"l2_lambda\": 0.01,\n",
    "    \"layers\": [128, 64, 32, 64, 128],\n",
    "    \"activation\": \"ReLU\",\n",
    "}\n",
    "\n",
    "csv_path = \"data/housing_prices/housing_prices.csv\"\n",
    "target_column = \"price\"\n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Experiment with Synthetic Data (Augmented Dataset)\n",
    "\n",
    "In this experiment, synthetic data was added to the dataset with 1000 samples, and the following configuration was used:\n",
    "\n",
    "- 5000 epochs (extended training duration)\n",
    "\n",
    "- Learning rate: 0.01 (kept the same)\n",
    "\n",
    "- Dropout rate: 0.5 (kept the same)\n",
    "\n",
    "- L2 regularization: 0.01 (kept the same)\n",
    "\n",
    "- Layers: [128, 64, 32]\n",
    "\n",
    "- ReLU activation function\n",
    "\n",
    "The hypothesis was to see if synthetic data would improve the model performance, but the results suggest that the added data does not significantly improve the model's predictions compared to the previous experiment with the original dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: 1000 epochs, 3 layers, ReLU activation, synthetic data with 1000 samples\n",
    "# This works the best. \n",
    "# Note: Adding synthetic data doesn't seem to help much.\n",
    "\n",
    "training_config = {\n",
    "        \"epochs\": 5000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64, 32],\n",
    "        \"activation\": \"ReLU\"\n",
    "    }\n",
    "\n",
    "csv_path = \"data/housing_prices/augmented_housing_prices_large.csv\"\n",
    "target_column = \"price\"  \n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "\n",
    "The app for predicting housing prices using a neural network offers an effective solution for real-estate analysis by providing insights into price predictions based on historical data. The experiments conducted have shown the following:\n",
    "\n",
    "### Model Performance:\n",
    "\n",
    "- The model performs optimally with 1000 epochs and 3 hidden layers (128, 64, 32) using a ReLU activation function. This configuration produced the best predictive results with low error rates, making it the preferred setup.\n",
    "\n",
    "### Training Configurations:\n",
    "\n",
    "- Increasing the epochs to 5000 or adding more layers did not significantly enhance performance. This indicates that the model reaches an optimal capacity at a certain configuration, and further complexity does not always equate to better predictions.\n",
    "\n",
    "### Synthetic Data Usage:\n",
    "\n",
    "- Adding synthetic data with 1000 additional samples did not noticeably improve performance. The original dataset provided sufficient variability and patterns for accurate predictions, suggesting that more data may not always be the solution unless it's of higher quality or better representative of the real world.\n",
    "\n",
    "### App Takeaways:\n",
    "\n",
    "- The app efficiently predicts housing prices, utilizing a trained neural network to generate accurate results based on given features.\n",
    "\n",
    "- The best configuration for training the model involves 3 layers and 1000 epochs, providing a balance of training time and accuracy.\n",
    "\n",
    "- The app can be extended further with additional data preprocessing, feature engineering, or deployment capabilities to handle larger datasets and improve prediction outcomes over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! ðŸŒ\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
