{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot Readme Generator (WIP)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://languageicon.org/language-icon.png\"> \n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "\n",
    "This app automates the generation of structured README files by creating a table of contents, integrating badges, and providing customizable configurations for headers, sections, and taglines. It uses a large language model (LLM) to tailor the content to specific project needs and ensures optimal formatting and professionalism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Installation\n",
    "\n",
    "This step installs required dependencies using install_requirements() and loads essential environment variables via setup_env(). It ensures all configurations, like OPENAI_API_KEY, are properly set up for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"üöÄ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the EasyLLM Class and Its Methods\n",
    "\n",
    "This EasyLLM class is a Python wrapper around OpenAI's GPT API that simplifies interactions with the model. It includes methods for generating text, handling responses as objects, managing model selection, and estimating token usage.\n",
    "\n",
    "By using EasyLLM, you can easily interact with OpenAI's GPT models without needing to manually handle API requests, set up system prompts, or process model responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "import re\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! üöÄ\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! ‚ö°Ô∏è\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate the number of tokens in a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string.\n",
    "\n",
    "        Returns:\n",
    "            int: Estimated token count.\n",
    "        \"\"\"\n",
    "        # Split text by whitespace and count tokens, including punctuation\n",
    "        tokens = re.findall(r\"\\S+\", text)\n",
    "        return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Classes, Models, and Functions for README Generation\n",
    "\n",
    " This step sets up classes, models, and functions to automate README generation. \n",
    "\n",
    "It includes defining project details, extracting badges, generating a dynamic table of contents, and configuring headers. \n",
    "\n",
    "The modular approach ensures flexibility, enabling well-structured and comprehensive READMEs tailored to each project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import requests\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Project(BaseModel):\n",
    "    repo_name: str\n",
    "    title: str\n",
    "    description: str\n",
    "    contributors: list[str]\n",
    "    references: list[str]\n",
    "    tech_stack: list[str]\n",
    "    tags: list[str]\n",
    "    license: str\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: list[str]\n",
    "\n",
    "\n",
    "def extract_badges_from_markdown(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract badges and their URLs from the given markdown text.\n",
    "\n",
    "    Args:\n",
    "        markdown_text (str): The markdown content containing badges.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[!\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)\\]\"\n",
    "    matches = re.findall(pattern, markdown_text)\n",
    "    return {badge: url for badge, url in matches}\n",
    "\n",
    "\n",
    "def get_badges() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get the markdown content of the README file containing badges.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/inttter/md-badges/refs/heads/main/README.md\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    badges_markdown = response.text\n",
    "    return extract_badges_from_markdown(badges_markdown)\n",
    "\n",
    "\n",
    "def get_table_of_contents(llm: EasyLLM, project: Project) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates the table of contents for the README.\n",
    "\n",
    "    Args:\n",
    "        llm (EasyLLM): The EasyLLM instance.\n",
    "        project (Project): The Project instance.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sections for the table of contents.\n",
    "    \"\"\"\n",
    "    mandatory_sections_first_half = [\n",
    "        \"About\",\n",
    "        \"Problem Statement\",\n",
    "        \"Research Areas\",\n",
    "        \"Installation\",\n",
    "        \"Usage\",\n",
    "    ]\n",
    "    mandatory_sections_second_half = [\n",
    "        \"Contributing\",\n",
    "        \"License\",\n",
    "        \"Acknowledgements\",\n",
    "        \"Authors\",\n",
    "        \"References\",\n",
    "    ]\n",
    "    table_of_contents = mandatory_sections_first_half\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Given the table of contents for the GitHub README of Project: '{project.title}' with the following sections:\n",
    "        Mandatory Sections: {json.dumps(mandatory_sections_first_half + mandatory_sections_second_half)}\n",
    "        These sections should be linked to the respective sections in the README.\n",
    "        These are mandatory sections for a good README.\n",
    "        You can add more sections if you want as per the project requirements.\n",
    "        For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "        Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "\n",
    "        Generate any 3 new sections unique, and specific to the project.\n",
    "        Make sure you don't repeat the same sections and don't include the mandatory sections again.\n",
    "\n",
    "        Project Details: '''{project.model_dump_json()}''''\n",
    "    \"\"\"\n",
    "    sections = llm.generate_object(prompt, Sections)\n",
    "\n",
    "    if sections and len(sections.sections) > 0:\n",
    "        for section in sections.sections:\n",
    "            if section not in table_of_contents:\n",
    "                table_of_contents.append(section)\n",
    "\n",
    "    for section in mandatory_sections_second_half:\n",
    "        if section not in table_of_contents:\n",
    "            table_of_contents.append(section)\n",
    "\n",
    "    return table_of_contents\n",
    "\n",
    "\n",
    "def get_readme_generator_config(title: str) -> Dict[str, Any]:\n",
    "    README_GENERATOR_CONFIG = {\n",
    "        \"visual_config\": {\n",
    "            \"header\": {\n",
    "                \"prompt\": f\"\"\"\n",
    "                Generate a good header section for the README\n",
    "                1. TITLE: It should contain the title of the project with an emoji. \n",
    "                Example: `# üöÄ My Awesome Project`\n",
    "                Be creative with the emoji and title and try to ensure that the title is relevant to the project and README.\n",
    "                TITLE: {title}\n",
    "\n",
    "                2. BADGES: After the title add badges.\n",
    "                Available BadgesL {json.dumps(get_badges())}   \n",
    "\n",
    "                3. Message to star us on GitHub.\n",
    "                Example: Please give us a ‚≠ê on GitHub if this project helped you!\n",
    "\n",
    "                4. Marketing Tagline: Add a marketing tagline for the project.\n",
    "                Example: #### Simplify your ETL pipelines with LitETL üî• ‚Äî the lightweight ETL framework!\n",
    "                \"\"\"\n",
    "            },\n",
    "            \"table_of_contents\": {\n",
    "                \"prompt\": \"\"\"\n",
    "                Generate a table of contents for the README.\n",
    "                This should include \"About\", \"Problem Statement\", \"Research Areas\", \"Installation\", \"Usage\", \"Contributing\", \"License\", \"Acknowledgements\", \"Authors\", \"References\".\n",
    "                These sections should be linked to the respective sections in the README.\n",
    "                These are mandatory sections for a good README.\n",
    "                You can add more sections if you want as per the project requirements.\n",
    "                For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "                Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "                \"\"\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    return README_GENERATOR_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract and Process Badges for README\n",
    "\n",
    "Badges are extracted from the markdown content, converted into a well-formatted JSON string, their token usage is estimated, and both the badge count and the formatted JSON representation are displayed for review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "llm = EasyLLM()\n",
    "badges = get_badges()\n",
    "badges_json = json.dumps(badges, indent=2)\n",
    "\n",
    "\n",
    "print(f\"Badges found in the README: {len(badges)}\")\n",
    "# For estimating if it will fit in the LLM context window\n",
    "print(f\"Tokens: {llm.estimate_tokens(badges_json)}\")\n",
    "print(badges_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5: Generate Table of Contents for README\n",
    "\n",
    " This step focuses on generating a dynamic table of contents for the README. By utilizing the EasyLLM instance and a Project object, it customizes the sections of the ToC based on both mandatory and project-specific inputs. \n",
    "\n",
    "The table of contents is generated, and the output is cleaned up to provide a clean and readable display before printing the final list of sections to be included in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "table_of_contents = get_table_of_contents(\n",
    "    llm,\n",
    "    Project(\n",
    "        repo_name=\"test\",\n",
    "        title=\"Test\",\n",
    "        description=\"Test\",\n",
    "        contributors=[\"Adi\"],\n",
    "        references=[\"Ref\"],\n",
    "        tech_stack=[\"Python\"],\n",
    "        tags=[\"Test\"],\n",
    "        license=\"MIT\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Table of Contents: {table_of_contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Implementing the EasyLLM Class\n",
    "\n",
    "The EasyLLM class provides a streamlined interface for interacting with the OpenAI API. \n",
    "\n",
    "It offers methods for generating text, parsing structured objects, managing model configurations, and estimating token usage. \n",
    "\n",
    "The class simplifies the process of text generation and provides utilities to manage prompts, handle errors, and estimate the computational load in terms of tokens, ensuring efficient use of the OpenAI model. \n",
    "\n",
    "By initializing with necessary configurations, it facilitates tasks like dynamic text generation and response parsing with minimal setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "import re\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! üöÄ\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! ‚ö°Ô∏è\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate the number of tokens in a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string.\n",
    "\n",
    "        Returns:\n",
    "            int: Estimated token count.\n",
    "        \"\"\"\n",
    "        # Split text by whitespace and count tokens, including punctuation\n",
    "        tokens = re.findall(r\"\\S+\", text)\n",
    "        return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate README Configuration and Content\n",
    "\n",
    "In Step 7, the process involves extracting badges from a markdown file, generating a table of contents, and configuring the README for a project. The Project class holds the project details, while the Sections class organizes the README sections. \n",
    "\n",
    "The functions `extract_badges_from_markdown` and `get_badges fetch` and `process badges`, and `get_table_of_contents` uses a custom LLM to generate a dynamic table of contents. \n",
    "\n",
    "The `get_readme_generator_config` function then compiles the visual configuration, including badges, header, and table of contents. The result is a detailed, structured README ready for use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import requests\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Project(BaseModel):\n",
    "    repo_name: str\n",
    "    title: str\n",
    "    description: str\n",
    "    contributors: list[str]\n",
    "    references: list[str]\n",
    "    tech_stack: list[str]\n",
    "    tags: list[str]\n",
    "    license: str\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: list[str]\n",
    "\n",
    "\n",
    "def extract_badges_from_markdown(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract badges and their URLs from the given markdown text.\n",
    "\n",
    "    Args:\n",
    "        markdown_text (str): The markdown content containing badges.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[!\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)\\]\"\n",
    "    matches = re.findall(pattern, markdown_text)\n",
    "    return {badge: url for badge, url in matches}\n",
    "\n",
    "\n",
    "def get_badges() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get the markdown content of the README file containing badges.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/inttter/md-badges/refs/heads/main/README.md\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    badges_markdown = response.text\n",
    "    return extract_badges_from_markdown(badges_markdown)\n",
    "\n",
    "\n",
    "def get_table_of_contents(llm: EasyLLM, project: Project) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates the table of contents for the README.\n",
    "\n",
    "    Args:\n",
    "        llm (EasyLLM): The EasyLLM instance.\n",
    "        project (Project): The Project instance.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sections for the table of contents.\n",
    "    \"\"\"\n",
    "    mandatory_sections_first_half = [\n",
    "        \"About\",\n",
    "        \"Problem Statement\",\n",
    "        \"Research Areas\",\n",
    "        \"Installation\",\n",
    "        \"Usage\",\n",
    "    ]\n",
    "    mandatory_sections_second_half = [\n",
    "        \"Contributing\",\n",
    "        \"License\",\n",
    "        \"Acknowledgements\",\n",
    "        \"Authors\",\n",
    "        \"References\",\n",
    "    ]\n",
    "    table_of_contents = mandatory_sections_first_half\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Given the table of contents for the GitHub README of Project: '{project.title}' with the following sections:\n",
    "        Mandatory Sections: {json.dumps(mandatory_sections_first_half + mandatory_sections_second_half)}\n",
    "        These sections should be linked to the respective sections in the README.\n",
    "        These are mandatory sections for a good README.\n",
    "        You can add more sections if you want as per the project requirements.\n",
    "        For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "        Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "\n",
    "        Generate any 3 new sections unique, and specific to the project.\n",
    "        Make sure you don't repeat the same sections and don't include the mandatory sections again.\n",
    "\n",
    "        Project Details: '''{project.model_dump_json()}''''\n",
    "    \"\"\"\n",
    "    sections = llm.generate_object(prompt, Sections)\n",
    "\n",
    "    if sections and len(sections.sections) > 0:\n",
    "        for section in sections.sections:\n",
    "            if section not in table_of_contents:\n",
    "                table_of_contents.append(section)\n",
    "\n",
    "    for section in mandatory_sections_second_half:\n",
    "        if section not in table_of_contents:\n",
    "            table_of_contents.append(section)\n",
    "\n",
    "    return table_of_contents\n",
    "\n",
    "\n",
    "def get_readme_generator_config(title: str) -> Dict[str, Any]:\n",
    "    README_GENERATOR_CONFIG = {\n",
    "        \"visual_config\": {\n",
    "            \"header\": {\n",
    "                \"prompt\": f\"\"\"\n",
    "                Generate a good header section for the README\n",
    "                1. TITLE: It should contain the title of the project with an emoji. \n",
    "                Example: `# üöÄ My Awesome Project`\n",
    "                Be creative with the emoji and title and try to ensure that the title is relevant to the project and README.\n",
    "                TITLE: {title}\n",
    "\n",
    "                2. BADGES: After the title add badges.\n",
    "                Available BadgesL {json.dumps(get_badges())}   \n",
    "\n",
    "                3. Message to star us on GitHub.\n",
    "                Example: Please give us a ‚≠ê on GitHub if this project helped you!\n",
    "\n",
    "                4. Marketing Tagline: Add a marketing tagline for the project.\n",
    "                Example: #### Simplify your ETL pipelines with LitETL üî• ‚Äî the lightweight ETL framework!\n",
    "                \"\"\"\n",
    "            },\n",
    "            \"table_of_contents\": {\n",
    "                \"prompt\": \"\"\"\n",
    "                Generate a table of contents for the README.\n",
    "                This should include \"About\", \"Problem Statement\", \"Research Areas\", \"Installation\", \"Usage\", \"Contributing\", \"License\", \"Acknowledgements\", \"Authors\", \"References\".\n",
    "                These sections should be linked to the respective sections in the README.\n",
    "                These are mandatory sections for a good README.\n",
    "                You can add more sections if you want as per the project requirements.\n",
    "                For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "                Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "                \"\"\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    return README_GENERATOR_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Retrieve and Format Badges, Estimate Token Usage\n",
    "\n",
    "In this step, the process revolves around retrieving badges from a markdown file, formatting them into JSON, and estimating token usage.\n",
    "\n",
    "The code initiates an EasyLLM instance to interact with the language model, retrieves badges through `get_badges()`, and converts them into a JSON string using `json.dumps()`. The number of badges is printed to provide insight into the extraction process, and token usage is estimated using the `estimate_tokens()` method to ensure the resulting data fits within the model's token limit. Finally, the JSON representation of the badges is displayed for clarity.\n",
    "\n",
    "This workflow ensures that badge data is extracted, formatted, and optimized for processing with the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "llm = EasyLLM()\n",
    "badges = get_badges()\n",
    "badges_json = json.dumps(badges, indent=2)\n",
    "\n",
    "\n",
    "print(f\"Badges found in the README: {len(badges)}\")\n",
    "# For estimating if it will fit in the LLM context window\n",
    "print(f\"Tokens: {llm.estimate_tokens(badges_json)}\")\n",
    "print(badges_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generating Table of Contents for README\n",
    "\n",
    "In Step 9, the code focuses on generating and displaying a table of contents for a README file using a Project instance and a language model (LLM).\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **`clear_output()`**: Clears the output in the Jupyter Notebook to keep the interface clean and organized.\n",
    "\n",
    "- **`get_table_of_contents()`**: This function generates the table of contents using a Project instance. The Project instance includes attributes like `repo_name`, `title`, `contributors`, and others. The `llm` instance helps suggest additional sections based on the project‚Äôs content.\n",
    "\n",
    "- **Printing the Table of Contents**: After generating the table of contents, it is printed to show the sections that should be included in the README.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The objective is to dynamically generate a structured list of sections for the README, ensuring that the content is tailored to the specific project. This process enhances the organization and clarity of the notebook's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "table_of_contents = get_table_of_contents(\n",
    "    llm,\n",
    "    Project(\n",
    "        repo_name=\"test\",\n",
    "        title=\"Test\",\n",
    "        description=\"Test\",\n",
    "        contributors=[\"Adi\"],\n",
    "        references=[\"Ref\"],\n",
    "        tech_stack=[\"Python\"],\n",
    "        tags=[\"Test\"],\n",
    "        license=\"MIT\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Table of Contents: {table_of_contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "This application streamlines the process of generating structured and informative README files for projects by automating key components such as the table of contents, badge extraction, and visual configuration. By leveraging a large language model (LLM) and predefined templates, the app ensures that each README is comprehensive and aligned with best practices.\n",
    "\n",
    "- ### Key highlights of the app:\n",
    "\n",
    "    - `Automated Table of Contents Generation`: It dynamically creates a table of contents based on mandatory and custom sections, tailored to each project's specific needs.\n",
    "    \n",
    "    - `Badge Integration`: The app can fetch badges from external markdown sources and include them in the README for a more polished presentation.\n",
    "\n",
    "    - `Customizable Configuration`: Through configurable prompts, it generates headers, section titles, and marketing taglines, ensuring a professional and engaging README.\n",
    "\n",
    "    - `Token Estimation for LLM`: The app estimates token usage for large data to prevent issues with token limits when working with the LLM.\n",
    "    \n",
    "Overall, this app significantly reduces manual effort, enhances the quality of README files, and provides a consistent and automated way to document projects effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
